{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "114502ac-2ffe-4ab2-ae08-3f2113a20792",
   "metadata": {},
   "source": [
    "# Generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5n3dh82l-sebu-5okn-vke9-2fnli2bl7ylr",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \r\n",
    "def reweight_distribution(original_distribution, temperature=0.5):\r\n",
    "    distribution = np.log(original_distribution) / temperature\r\n",
    "    distribution = np.exp(distribution)\r\n",
    "    return distribution / np.sum(distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "q36nango-vbml-ay0l-7hh7-zkpg7lfquabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 12:03:58.938832: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-27 12:03:59.027954: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732701839.063509    6152 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732701839.073777    6152 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-27 12:03:59.156848: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100006 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1732701841.829185    6152 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \r\n",
    "from tensorflow import keras\r\n",
    "tf.TF_ENABLE_ONEDNN_OPTS=0\r\n",
    "dataset = keras.utils.text_dataset_from_directory(\r\n",
    "    directory=\"aclImdb\", label_mode=None, batch_size=256)\r\n",
    "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"<br />\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ipyngxc1-kg3f-d1bj-aogk-y38jhy4tu0z7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\r\n",
    "\r\n",
    "keras.config.disable_traceback_filtering() \r\n",
    "class PositionalEmbedding(layers.Layer):\r\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):  \r\n",
    "        super().__init__(**kwargs)\r\n",
    "        self.token_embeddings = layers.Embedding(                          \r\n",
    "            input_dim=input_dim, output_dim=output_dim)\r\n",
    "        self.position_embeddings = layers.Embedding(\r\n",
    "            input_dim=sequence_length, output_dim=output_dim)              \r\n",
    "        self.sequence_length = sequence_length\r\n",
    "        self.input_dim = input_dim\r\n",
    "        self.output_dim = output_dim\r\n",
    "  \r\n",
    "    def call(self, inputs):\r\n",
    "        length = tf.shape(inputs)[-1]\r\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\r\n",
    "        embedded_tokens = self.token_embeddings(inputs)\r\n",
    "        embedded_positions = self.position_embeddings(positions)\r\n",
    "        return embedded_tokens + embedded_positions            def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)    return inputs != 0\r\n",
    " \r\n",
    "    def get_config(self):\r\n",
    "        config = super().get_config()\r\n",
    "        config.update({\r\n",
    "            \"output_dim\": self.output_dim,\r\n",
    "            \"sequence_length\": self.sequence_length,\r\n",
    "            \"input_dim\": self.input_dim,\r\n",
    "        })\r\n",
    "        return config\r\n",
    "\r\n",
    "\r\n",
    "class TransformerDecoder(layers.Layer):\r\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\r\n",
    "        super().__init__(**kwargs)\r\n",
    "        self.embed_dim = embed_dim\r\n",
    "        self.dense_dim = dense_dim\r\n",
    "        self.num_heads = num_heads\r\n",
    "        self.attention_1 = layers.MultiHeadAttention(\r\n",
    "            num_heads=num_heads, key_dim=embed_dim)\r\n",
    "        self.attention_2 = layers.MultiHeadAttention(\r\n",
    "            num_heads=num_heads, key_dim=embed_dim)\r\n",
    "        self.dense_proj = keras.Sequential(\r\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\r\n",
    "             layers.Dense(embed_dim),]\r\n",
    "        )\r\n",
    "        self.layernorm_1 = layers.LayerNormalization()\r\n",
    "        self.layernorm_2 = layers.LayerNormalization()\r\n",
    "        self.layernorm_3 = layers.LayerNormalization()\r\n",
    "        self.supports_masking = True                     \r\n",
    "  \r\n",
    "    def get_config(self):\r\n",
    "        config = super().get_config()\r\n",
    "        config.update({\r\n",
    "            \"embed_dim\": self.embed_dim,\r\n",
    "            \"num_heads\": self.num_heads,\r\n",
    "            \"dense_dim\": self.dense_dim,\r\n",
    "        })\r\n",
    "        return config\r\n",
    "\r\n",
    "    def get_causal_attention_mask(self, inputs):\r\n",
    "        input_shape = tf.shape(inputs)\r\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\r\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\r\n",
    "        j = tf.range(sequence_length)\r\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")                           \r\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))    \r\n",
    "        mult = tf.concat(                                               \r\n",
    "            [tf.expand_dims(batch_size, -1),                            \r\n",
    "                tf.constant([1, 1], dtype=tf.int32)], axis=0)              \r\n",
    "        return tf.tile(mask, mult)          \r\n",
    "    \r\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\r\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\r\n",
    "        padding_mask = None       \r\n",
    "        if mask is not None:                                       \r\n",
    "            padding_mask = tf.cast(                                \r\n",
    "                mask[:, tf.newaxis, :], dtype=\"int32\")             \r\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)   \r\n",
    "        attention_output_1 = self.attention_1(\r\n",
    "            query=inputs,\r\n",
    "            value=inputs,\r\n",
    "            key=inputs,\r\n",
    "            attention_mask=causal_mask)                            \r\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\r\n",
    "        attention_output_2 = self.attention_2(\r\n",
    "            query=attention_output_1,\r\n",
    "            value=encoder_outputs,\r\n",
    "            key=encoder_outputs,\r\n",
    "            attention_mask=padding_mask,                           \r\n",
    "        )\r\n",
    "        attention_output_2 = self.layernorm_2(\r\n",
    "            attention_output_1 + attention_output_2)\r\n",
    "        proj_output = self.dense_proj(attention_output_2)\r\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "qdg0vr3d-zb47-vx5c-teca-bmsqk6h0tct6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 12:04:13.550988: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\r\n",
    "  \r\n",
    "sequence_length = 100 \r\n",
    "vocab_size = 15000                            \r\n",
    "text_vectorization = TextVectorization(\r\n",
    "    max_tokens=vocab_size,                \r\n",
    "    output_mode=\"int\",                        \r\n",
    "    output_sequence_length=sequence_length,   \r\n",
    ")\r\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rgs3o330-0af4-u1lz-ot1h-oo3uh5x6bn4z",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lm_dataset(text_batch):\r\n",
    "    vectorized_sequences = text_vectorization(text_batch)    \r\n",
    "    x = vectorized_sequences[:, :-1]                         \r\n",
    "    y = vectorized_sequences[:, 1:]                          \r\n",
    "    return x, y\r\n",
    "  \r\n",
    "lm_dataset = dataset.map(prepare_lm_dataset, num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bkum5tav-asul-ib2n-ra7j-xr6ity1cmes6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256 \r\n",
    "latent_dim = 2048 \r\n",
    "num_heads = 2 \r\n",
    "  \r\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\r\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\r\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)\r\n",
    "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)       \r\n",
    "model = keras.Model(inputs, outputs)\r\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "esc9ybzq-2p8d-g9kj-mbhn-0oofp2sccr9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))    \r\n",
    "  \r\n",
    "def sample_next(predictions, temperature=1.0):                         \r\n",
    "    predictions = np.asarray(predictions).astype(\"float64\")\r\n",
    "    predictions = np.log(predictions) / temperature\r\n",
    "    exp_preds = np.exp(predictions)\r\n",
    "    predictions = exp_preds / np.sum(exp_preds)\r\n",
    "    probas = np.random.multinomial(1, predictions, 1)\r\n",
    "    return np.argmax(probas)\r\n",
    "  \r\n",
    "class TextGenerator(keras.callbacks.Callback):\r\n",
    "    def __init__(self,\r\n",
    "                 prompt,                                               \r\n",
    "                 generate_length,                                      \r\n",
    "                 model_input_length,\r\n",
    "                 temperatures=(1.,),                                   \r\n",
    "                 print_freq=1):\r\n",
    "        self.prompt = prompt\r\n",
    "        self.generate_length = generate_length\r\n",
    "        self.model_input_length = model_input_length\r\n",
    "        self.temperatures = temperatures\r\n",
    "        self.print_freq = print_freq\r\n",
    "  \r\n",
    "    def on_epoch_end(self, epoch, logs=None):\r\n",
    "        if (epoch + 1) % self.print_freq != 0:\r\n",
    "            return\r\n",
    "        for temperature in self.temperatures:\r\n",
    "            print(\"== Generating with temperature\", temperature)\r\n",
    "            sentence = self.prompt                                     \r\n",
    "            for i in range(self.generate_length):\r\n",
    "                tokenized_sentence = text_vectorization([sentence])    \r\n",
    "                predictions = self.model(tokenized_sentence)           \r\n",
    "                next_token = sample_next(predictions[0, i, :])         \r\n",
    "                sampled_token = tokens_index[next_token]               \r\n",
    "                sentence += \" \" + sampled_token                        \r\n",
    "            print(sentence)\r\n",
    "  \r\n",
    "prompt = \"This movie\"\r\n",
    "\r\n",
    "from keras.callbacks import ModelCheckpoint\r\n",
    "\r\n",
    "callbacks = [\r\n",
    "    ModelCheckpoint(filepath=\"text_gen.keras\", save_best_only=True, monitor=\"loss\"),\r\n",
    "    TextGenerator(prompt, generate_length=50,\r\n",
    "    model_input_length=sequence_length,\r\n",
    "    temperatures=(0.2, 0.5, 0.7, 1., 1.5)),\r\n",
    "]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44507e36-5c2c-4237-9bde-659c0b46e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"text_gen.keras\", custom_objects={\"PositionalEmbedding\": PositionalEmbedding, \"TransformerDecoder\": TransformerDecoder})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "544c8426-acf3-4650-9955-4f9b9e7e0b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This horror movie was movie movie was definitely was was was was was is was picked disappointed totally not  on scared stupid william keller the was was he\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This horror movie was\"                                  \n",
    "for i in range(25):\n",
    "    tokenized_sentence = text_vectorization([sentence])\n",
    "    predictions = model(tokenized_sentence)  \n",
    "    next_token = sample_next(predictions[0, i, :])         \n",
    "    sampled_token = tokens_index[next_token]               \n",
    "    sentence += \" \" + sampled_token                        \n",
    "print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lfd8b22i-av7i-d6u4-7zlb-popbnscjlhb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kassu/anaconda3/envs/keras-cpu/lib/python3.12/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/kassu/anaconda3/envs/keras-cpu/lib/python3.12/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/kassu/anaconda3/envs/keras-cpu/lib/python3.12/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 6.3881== Generating with temperature 0.2\n",
      "This movie is when born war in this the film rental festival ever ive be always written shot we it see was those turned beautiful out and at tv the series chance it to far have too been much a right 10 through crap the should perry inside show brand everything g\n",
      "== Generating with temperature 0.5\n",
      "This movie is was threatened all by let the the things hour have but been the filmed other in big his surviving life fantastic because back she in bitter special 19th effects queen [UNK] concorde having as laughable is usually wrong satisfying with satire years male commandments product people that if fourteen\n",
      "== Generating with temperature 0.7\n",
      "This movie is is the agreeing meantime although shows its good claudette way has quite written fact but that something their fun time if he you didnt ask describe if any you genuinely must device see stupid the acting actors youve bad recently music got character and today i dressed think regret\n",
      "== Generating with temperature 1.0\n",
      "This movie flick is received quite to big see imagery a deranged abyss movies it with is announced a our zombie problems tale that the noir films giant i [UNK] loved [UNK] less everyone than else lousy in to this the hey original some movies third of 90s versions the of slasher\n",
      "== Generating with temperature 1.5\n",
      "This movie is script [UNK] one frank of tell lloyd hoping dvd to original take film in but the unfortunately motivation we the might worst even movie promising it complain was the it actors when are it charlotte is actors [UNK] seem monster to is be still something denies predictable  and\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1094s\u001b[0m 3s/step - loss: 6.3869\n",
      "Epoch 2/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 5.5037== Generating with temperature 0.2\n",
      "This movie one is time searching is for merely themselves a for handful diverse of joke power of with an the obvious print yet and take i everything still to occurred be to going this to film learn without when feeling our at favorite least character works with like one a man\n",
      "== Generating with temperature 0.5\n",
      "This movie moore is scientist reminded who me still it the thats plot why or she nothing keeps else an to controversial stick just with bad the movies characters fit whereas for between some human pity con they band were shadow audrey of attack nonsense who a goes computer into we the\n",
      "== Generating with temperature 0.7\n",
      "This movie is chases for boss what creates happens this but character detailed aggressive elements catching the best philadelphia things is about great any and hardcore the actresses [UNK] its scooby grim silverman current and english at film a if deep you generation look that of is this good one then of\n",
      "== Generating with temperature 1.0\n",
      "This movie movie is but some it stupid holds it the entertainment middle half evoke of it overwhelmed completely and evident special used effects to for tell it us are at so least violent it about forced the to exception appear and to enduring us love this queens is angst extremely or\n",
      "== Generating with temperature 1.5\n",
      "This movie movie is are a hostile more showgirls intense movies a scene story without line turtle its this good is and about acts full sort of of attempt craving to maybe be its it shown into by order years to and keep they good surrounding films respect is to a the\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1091s\u001b[0m 3s/step - loss: 5.5036\n",
      "Epoch 3/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 5.3400== Generating with temperature 0.2\n",
      "This movie was is just written made you of right every to scene write to one get of a the sexist film rightly would until like now the the casting hero and seen see this it unbelievably was painful basically to doesnt catch seem a like horses most movie part convention when\n",
      "== Generating with temperature 0.5\n",
      "This movie movie is from terrible what i are wouldnt factors have every ever overdone seen and are blonde looking requires for you the [UNK] plot written pretending and to predictable make the up plot way just to couldnt watch afford it just from a where great generic 7 i actresses just\n",
      "== Generating with temperature 0.7\n",
      "This movie movie is showed deep up it everyone left isnt with whats only happening redeeming by qualities the to words the throughout acting the career greatest the story good with limited creepy effort part is is bad one yet of predictable cars and the action plot movie its others nice campy\n",
      "== Generating with temperature 1.0\n",
      "This movie movie is contains so two bad a now waste the your event seat the with situation a and superb quite acting another is day awful its the some music knowledge and i done couldnt but watch this and is the terrible effect the so drop incredibly here funny can the\n",
      "== Generating with temperature 1.5\n",
      "This movie channel is made one it of strongly a the very worked good as toe it there is is bad a watching wonderful the story best it actors was are what caught fans  dont sure if you are only bad its deserved tim mature and quite why quite terrible it\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1091s\u001b[0m 3s/step - loss: 5.3399\n",
      "Epoch 4/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 5.2371== Generating with temperature 0.2\n",
      "This movie show i i on hated so it expecting i my did [UNK] not it like and i i was had how showed couple it would it love i it have and never would saw first the i tension really and the most movies really this didnt movie know but this\n",
      "== Generating with temperature 0.5\n",
      "This movie movie now starts haired off first pretty couple but of had its potential characters but work as and only spirit eight decide father to at make earth an turn exotic it proceeds gives to the stay final on 20 it id had suggest ever that via i fashions suppose away\n",
      "== Generating with temperature 0.7\n",
      "This movie movie sucks so fun so it slow is and it love going it that i living wish 27 i people love believe it how was [UNK] literally as doesnt this spread movie along could i have think seemed she to acts say them something was i not really afraid sweet\n",
      "== Generating with temperature 1.0\n",
      "This movie movie was is i bad watch to values watch i it remember i done saw this it movie i i do must i feel thought bored and watching i how were it horrible was movie pay did i i didnt think like about i it was again bad to for\n",
      "== Generating with temperature 1.5\n",
      "This movie movie is looks odd good it flick seems i gripping really the about two violence years and ive the rented same it characterization doesnt minions seem coburn funny or but who when say it nothing wasnt to well be the it title but likes it souls begins manage to an\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1091s\u001b[0m 3s/step - loss: 5.2371\n",
      "Epoch 5/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 5.1605== Generating with temperature 0.2\n",
      "This movie movie is was just expecting not a bad horrible but everything there weve was all the the [UNK] turks and are the look game like made the it worst was movie i with liked the the movie plot why was they carried say them that emotions saw never bad did\n",
      "== Generating with temperature 0.5\n",
      "This movie is was one just of stupid my scary least but i now cant i believe dont it know and what thought that this i film say i that have anyway made without my scary surprise it me was if absolutely im lightly a charlie low wilson rent i i crap\n",
      "== Generating with temperature 0.7\n",
      "This movie movie did was look bad i so dont stopped know it but was i i for nearly no all flicks and last i half could hour watch i it watched its it just and watched the a acting 3 can seasons always 9 thought not it a didnt 10 have\n",
      "== Generating with temperature 1.0\n",
      "This movie movie just was werent so one bad of i my was daughter a made new in really classic bad disney film movies therefore there people was that believing delivered a with taste this you film if and you it have on seen you made would me watch something it like\n",
      "== Generating with temperature 1.5\n",
      "This movie movie is was fun only after actually reading out some that some i crappy loved action it the in worst a movie movie i it was that just can ridiculous tell i you am are catches actually it laughed and in its my a mother certain marching things before wrong\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1092s\u001b[0m 3s/step - loss: 5.1605\n",
      "Epoch 6/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 5.0995== Generating with temperature 0.2\n",
      "This movie movie was is stupid a playing total the type vampire of movies the that characters the are conversations the that characters between are two cheap girls and and lost the one ones really would begins not with a her machete could characters not are silly strong and character most at\n",
      "== Generating with temperature 0.5\n",
      "This movie movie cant was go someones nowhere carole at and waste acting of bad 10 acting heroes cheap and and never then have you been can through easily plot married convey 3 dialog times and when [UNK] it and barely attention any of of it [UNK] gets us your hero computer\n",
      "== Generating with temperature 0.7\n",
      "This movie movie was had a been wonderful different story way so better great to action its would overacting be to fascinating ruin like it most was sci the fi psychological done thriller [UNK] should still be a hard movie to are it great just music enjoying of this two is times\n",
      "== Generating with temperature 1.0\n",
      "This movie is was kind a that dazzling this game movie was which terrible sounds and too it bad already unfortunately been one a liners nice and if was you indeed did a some classic lovely screen in some the good directing monster you is cannot completely take fell some asleep odd\n",
      "== Generating with temperature 1.5\n",
      "This movie movie was give a it talk was i something even going written on watch about it budget sucks or i something dont original really and boring she and was wait wrong before it in would stitches have ben told it you had when been i funny had out no who\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1092s\u001b[0m 3s/step - loss: 5.0995\n",
      "Epoch 7/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 5.0422== Generating with temperature 0.2\n",
      "This movie movie sucked sucked horrible it script was was terrible absolutely and bad it online was it awful if movie it funny got just on sucked effects what bad the the idea worst it music its sucked basically was adding boring it but wasnt this its waste pretty 3 much whoever\n",
      "== Generating with temperature 0.5\n",
      "This movie movie it refreshing was movie worse wont than hurt what one is it not for scary you some might really like likes it goodness it it reflects was i quite not a even first scary bad movie either  great i cried at all their travelers might make sense to\n",
      "== Generating with temperature 0.7\n",
      "This movie movie bad is youll good doubt that kids you here are and supposed its to because improve its any just effort cant the even omega pg woman then who the hang two with soldiers ralph hmm [UNK] how but realistic ride thats dont root even for then oscar perhaps you\n",
      "== Generating with temperature 1.0\n",
      "This movie movie seriously up bad [UNK] bad [UNK] worse boring acting characterization bad awful acting 1 boring or bad boring acting acting bad bad horrid ruins music better bad movies acting bad bad script acting awful bad bad acting it awful bad story music bad was acting horrible painful this poor\n",
      "== Generating with temperature 1.5\n",
      "This movie movie absolutely this have movie seen a it bad is nothing worse action acting movie  is it falls slow it and thinking the first movie they will have made them after the hit over you dont believe i ran out  was worst scene it on i one probably\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1092s\u001b[0m 3s/step - loss: 5.0421\n",
      "Epoch 8/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 4.9879== Generating with temperature 0.2\n",
      "This movie movie movie monster action this there video horror i movie rented was this a twist bitter then ever this a movie terrible started there a were 10 only  is not rent a rating a horror movie a scary movies at a a a ten barely a this movie a\n",
      "== Generating with temperature 0.5\n",
      "This movie movie at on me a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
      "== Generating with temperature 0.7\n",
      "This movie movie really the made movie this definitely movie recommend horror a movies horror im movies a dont very think bad the horror make movie it a a great boring movie a it movie was bit a effects movie terrible i movie have  the plot was only movie but a\n",
      "== Generating with temperature 1.0\n",
      "This movie movie is is a a movie there this  movie make you is not  this this this this this this it snow this this this this this this this a this movie this it movie it it this video it this this it family it movie movie  movie\n",
      "== Generating with temperature 1.5\n",
      "This movie movie dont terrible make it it made movie me one a thing clear it be keeps intelligent you movie believe really 45 recommend  this movie comedy horror there is not one of the action movies every movie this movie dont horrible films maybe its true story this movie a\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1091s\u001b[0m 3s/step - loss: 4.9878\n",
      "Epoch 9/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 4.9343== Generating with temperature 0.2\n",
      "This movie movie movie look plot the right horror the the the emotion second the and last the the acting action was the and acting everything the complex childish the the dialogs the the the the many all nice the the way second the supposed effective horror the its the the effects\n",
      "== Generating with temperature 0.5\n",
      "This movie movie movie this this movie movie an this this movie movie movie movie it disinterested of the no you it this movie movie it movie is this what movie this the movie movie movie because it this said movie i will can is just money this is movie boring fits\n",
      "== Generating with temperature 0.7\n",
      "This movie movie this this movie movie this this movie movie a cost it this is is this really once is this this movie movie will this be movie if movie this it is is but movie it its isnt movies this movie movie this twice is is movie this is is\n",
      "== Generating with temperature 1.0\n",
      "This movie one is it this is is it it is is is this this is is is it is is is this  is this is is is this is this is it is is is this is this is this is is is this is is is rejects is is\n",
      "== Generating with temperature 1.5\n",
      "This movie movie movie movie out boring the scary action  suspense aspect or plot the storyline tons the budget are the acting the direction the everything the only the acting the this will ok the zero the excitement the sophomoric the movie the better not the action any some good the\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1092s\u001b[0m 3s/step - loss: 4.9342\n",
      "Epoch 10/10\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 4.8774== Generating with temperature 0.2\n",
      "This movie movie movie movie movie movie movie movie movie story movie movie movie movie movie  movie movie movie  movie movie this  movie movie movie  movie movie it horror movies movie movies older rent movie  this movie movie or movie movie movie movie movie movie  movie\n",
      "== Generating with temperature 0.5\n",
      "This movie movie movie movie movie movie action movie should this realize movie just movie movie movie  movie movie movie ever to for  movie recommend movie movie worst movie movie one movie its movie this  good watching  movie movie it it movie  watched movie  movie if\n",
      "== Generating with temperature 0.7\n",
      "This movie movie movie movie its scene movie in movie it movie movie movie this movie movie movie movie movie movie movie director my thriller movie movie movie movie on movie this movie movie in movie if movie movie movie  honest movie if since worse in parts   movie movie\n",
      "== Generating with temperature 1.0\n",
      "This movie movie movie one for family the movie movie movie movie movie this movies better movie movie  movie movie movie movie movie movie  movie movie movie movie scary  movie this movie movie movie movie  movie  movie movie the movie movie movie movie movie movies movie movies\n",
      "== Generating with temperature 1.5\n",
      "This movie movie movie movie movie movie  movie friendship movie watch movie movie movie  movie movie movie  movie you movie movie movie movie movie movie movie movie  movie movie  movie  movie movie movie movie movie movie movie movie movie movie discussion   movie  movie\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1092s\u001b[0m 3s/step - loss: 4.8773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x788da6fd7950>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(lm_dataset, epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edea9ba4-a505-4f59-978a-d603855095ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
