{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "qkqu8td5-z8zy-uclp-ib26-1ffvc9gfybvg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generatiivinen kielimalli\r\n",
    "\r\n",
    "## Tavoitteet\r\n",
    "Mallissa käytetään **imdb** elokuva-arvostelu sivun arvosteluja, joiden perusteella malli oppii generoimaan elokuva-arvosteluja. Generoidut arvostelu teivät tule olemaan järkeviä ja niissä on paljon kielellisiä virheitä, mutta mallin idea on vain demonstroida miten kyseinen malli tehtäisiin. Jos mallia kouluttaisi vielä pari päivää, se saataisiin varmasti generoimaan paljon tarkempia arvosteluja.\r\n",
    "## Datan kuvaus\r\n",
    "Datasetti pitää sisällään yli 170 000 elokuva-arvostelua, jotka on jaettu positiiivisiksi ja negatiivisiksi arvosteluiksi, kyseisistä luokista ei välitetä tämän mallin koulutuksessa. Data on tekstimuodossa, ja siitä pitää vain karsia muutamia merkkejä."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "h1cbxu2s-oymr-y1it-ytvg-u24eh72r539h",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Datan esikäsittely\r\n",
    "Kielimallit ovat hyvin rankkoja malleja kouluttaa `CPU:lla`, joten otamme `GPU` koulutuksen käyttöön, jos laite sitä tukee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72bf73a6-58d0-44fb-828a-e704d7301e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "0aaeixd0-2lyn-5xoh-nqqy-lfoub3kpgh8s",
   "metadata": {},
   "outputs": [],
   "source": [
    "Luomme oman mapping funktion, jolla käsittelemme `imbd` arvostelu tekstit. Mapping poistaa kaikki `<br />` merkit tekstistä, eli kaikki rivinvaihdot, jotka korvataan välilyönnillä. Poistamme myös kaikki unicode merkin komennolla `[^\\\\x00-\\\\x7F]+`. Unicode merkit poistetaan, koska laitetta jolla malli on koulutettu käyttää vanhaa versiota **kerasista**, joka ei tue `utf-8` merkkejä."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2d45806-ffeb-47ce-99c7-d6b06ca4ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(x):\n",
    "    x = tf.strings.regex_replace(x, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(x, \"[^\\x00-\\x7F]+\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "8gii6cmf-u45v-ga6e-rxhq-somerrx1qa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ladataan datasetti, jonka jälkeen arvostelut viedään `mapping` funktion läpi, joka poistaa rivinvaihdon ja `utf-8` merkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "q36nango-vbml-ay0l-7hh7-zkpg7lfquabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100006 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "tf.TF_ENABLE_ONEDNN_OPTS=0\n",
    "dataset = keras.utils.text_dataset_from_directory(\n",
    "    directory=\"aclImdb\", label_mode=None, batch_size=256)\n",
    "dataset = dataset.map(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "eyp45ulf-aeyb-d4p4-a3ad-6omes996eail",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mallinnus\r\n",
    "Luodaan `PositionalEmbedding` ja `TransformerDecoder` luokat. \r\n",
    "`PositionalEmbedding` luokan ideana on toimia `Embedding`-kerroksena, joka enkoodaa tokenit ja niiden sijainnit sanassa. Sijanti on tärkeää tietää lauseiden luonnissa, koska sanojen sijainnit lauseessa voi vaikuttaa suuresti sanan tarkoitukseen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ipyngxc1-kg3f-d1bj-aogk-y38jhy4tu0z7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):  \n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(                          \n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)              \n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "  \n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions                        \n",
    " \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    " \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True                     \n",
    "  \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")                           \n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))    \n",
    "        mult = tf.concat(                                               \n",
    "            [tf.expand_dims(batch_size, -1),                            \n",
    "                tf.constant([1, 1], dtype=tf.int32)], axis=0)              \n",
    "        return tf.tile(mask, mult)          \n",
    "    \n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        padding_mask = None       \n",
    "        if mask is not None:                                       \n",
    "            padding_mask = tf.cast(                                \n",
    "                mask[:, tf.newaxis, :], dtype=\"int32\")             \n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)   \n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask)                            \n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=attention_output_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,                           \n",
    "        )\n",
    "        attention_output_2 = self.layernorm_2(\n",
    "            attention_output_1 + attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "p4ysist3-kvgo-08ps-6ti4-kznc2ig2sdg6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vektorisoimme tekstin, joka tuottaa  `15 000:nen` kokoisen sanakirjan eniten käytetyistä sanoista. Asetamme myös esimerkki arvostelujen maksimipituudeksi `100` tokenia. Tämä ei tarkoita `100 merkkiä`, vaan `100 sanaa`. Sanat jotka eivät mahtuneet sanakirjaan mukaan tullaan merkkaamaan `[unk]` tekstinä mallin generaatioissa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qdg0vr3d-zb47-vx5c-teca-bmsqk6h0tct6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "  \n",
    "sequence_length = 100 \n",
    "vocab_size = 15000                            \n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,                \n",
    "    output_mode=\"int\",                        \n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "a8za1140-jvfa-sa5h-gy9o-pn9xkuui8umd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Seuraavassa solussa määritämme funktion, jonka avulla muutamme datasetin sellaiseksi, että voimme alkaa kouluttamaan sitä valitsemallamme mallilla. Funktiolla valitsemme vektorisoidusta lauseesta kaiken paitsi viimeisen arvon, jonka tallennamme `x`-muuttujaan. Valitsimme sitten kaikki paitsi ensimmäisen arvon, ja tallennamme sen `y`-muuttujaan. Funktio palauttaa sitten x:n ja y:n.\r\n",
    "Käymme sitten alkuperäisen datasetin läpi `.map`-komennolla jossa käytämme `prepare_lm_dataset`-funktiota joka välissä."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rgs3o330-0af4-u1lz-ot1h-oo3uh5x6bn4z",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lm_dataset(text_batch):\r\n",
    "    vectorized_sequences = text_vectorization(text_batch)    \r\n",
    "    x = vectorized_sequences[:, :-1]                         \r\n",
    "    y = vectorized_sequences[:, 1:]                          \r\n",
    "    return x, y\r\n",
    "  \r\n",
    "lm_dataset = dataset.map(prepare_lm_dataset, num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "tui70jaj-2xx8-xdgp-8f8d-84fc9ers14un",
   "metadata": {},
   "outputs": [],
   "source": [
    "Luomme nyt aikaisemmin määritetyillä `PositionalEmbedding` ja `TransformerDecoder` kerroksilla mallin. Malli koostuu input-kerroksesta, `PositionalEmbedding`-kerroksesta, `TransformerDecoder`-kerroksesta, ja lopulta mallissa on `Dense`-kerros softmax-aktivaatiolla, joka toimii output-kerroksena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkum5tav-asul-ib2n-ra7j-xr6ity1cmes6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256 \r\n",
    "latent_dim = 2048 \r\n",
    "num_heads = 2 \r\n",
    "\r\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\r\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\r\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)\r\n",
    "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)       \r\n",
    "model = keras.Model(inputs, outputs)\r\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "0891hepz-0d0f-uj64-m2k6-yrv960btt6tz",
   "metadata": {},
   "outputs": [],
   "source": [
    "Luomme `tokens_index` sanakirjan, jonka **avain** on `token index` ja **arvona** on sana, johon tokeni viittaa.\r\n",
    "\r\n",
    "`sample_next` funktio on tärkeässä osassa lauseen luonnissa, sitä kutsutaan loopissa kokoamaan lause yksi sana kerrallaan, kunnes lause on rakennettu. Funktio ottaa sisäänsä todennäköisimpiä sanoja lausetta rakentaessa, josta se valitsee `lämpötila` arvon perusteella satunnaisesti todennäköisemmän tokeni indeksin (mitä isompi **lämpötila**, sitä isompi hajonta sanojen välillä on). Funktio palauttaa kyseisen indeksin, jonka sana arvo haetaan `tokens_index` sanakirjasta ja lisätään lauseen perään.\r\n",
    "\r\n",
    "`TextGenerator` luokkaa käytetään jokaisen epochin jälkeen `callback` koukussa luomaan esimerkkitekstejä. Tämä ei vaikuta mitenkään mallin koulutukseen ja helpottaa vain mallin arviointia koulutuksen aikana.\r\n",
    "\r\n",
    "Tallennamme myös mallin tiedostoon `text_gen.keras` käyttämällä `ModelCheckpoint` luokkaa. Tämä mahdollistaa mallin uudelleen käytön myöhemmin, lataamalla kaikki painoarvot tiedostosta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "esc9ybzq-2p8d-g9kj-mbhn-0oofp2sccr9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))    \r\n",
    "  \r\n",
    "def sample_next(predictions, temperature=1.0):                         \r\n",
    "    predictions = np.asarray(predictions).astype(\"float64\")\r\n",
    "    predictions = np.log(predictions) / temperature\r\n",
    "    exp_preds = np.exp(predictions)\r\n",
    "    predictions = exp_preds / np.sum(exp_preds)\r\n",
    "    probas = np.random.multinomial(1, predictions, 1)\r\n",
    "    return np.argmax(probas)\r\n",
    "  \r\n",
    "class TextGenerator(keras.callbacks.Callback):\r\n",
    "    def __init__(self,\r\n",
    "                 prompt,                                               \r\n",
    "                 generate_length,                                      \r\n",
    "                 model_input_length,\r\n",
    "                 temperatures=(1.,),                                   \r\n",
    "                 print_freq=1):\r\n",
    "        self.prompt = prompt\r\n",
    "        self.generate_length = generate_length\r\n",
    "        self.model_input_length = model_input_length\r\n",
    "        self.temperatures = temperatures\r\n",
    "        self.print_freq = print_freq\r\n",
    "  \r\n",
    "    def on_epoch_end(self, epoch, logs=None):\r\n",
    "        if (epoch + 1) % self.print_freq != 0:\r\n",
    "            return\r\n",
    "        for temperature in self.temperatures:\r\n",
    "            print(\"== Generating with temperature\", temperature)\r\n",
    "            sentence = self.prompt                                     \r\n",
    "            for i in range(self.generate_length):\r\n",
    "                tokenized_sentence = text_vectorization([sentence])    \r\n",
    "                predictions = self.model(tokenized_sentence)           \r\n",
    "                next_token = sample_next(predictions[0, i, :], temperature)         \r\n",
    "                sampled_token = tokens_index[next_token]               \r\n",
    "                sentence += \" \" + sampled_token                        \r\n",
    "            print(sentence)\r\n",
    "  \r\n",
    "prompt = \"This movie\"\r\n",
    "\r\n",
    "from keras.callbacks import ModelCheckpoint\r\n",
    "\r\n",
    "callbacks = [\r\n",
    "    ModelCheckpoint(filepath=\"text_gen.keras\", save_best_only=True, monitor=\"loss\"),\r\n",
    "    TextGenerator(prompt, generate_length=50,\r\n",
    "    model_input_length=sequence_length,\r\n",
    "    temperatures=(0.2, 0.5, 0.7, 1., 1.5)),\r\n",
    "]     "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "xlsme2ok-9nac-0a6n-13ha-bmzuacjchv5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Seuraavaksi on mallin koulutus. Annamme mallille `lm_dataset` koulutusdatan, sekä ylhäällä määrittämämme kaksi eri `callback` kutsua. Kyseistä mallia on koulutettu `186` epochia, joka johtuu vain ajasta. Ideaalisti mallia pitäisi vielä kouluttaa muutama sata epochia, jotta sein saisi paremmaksi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "lfd8b22i-av7i-d6u4-7zlb-popbnscjlhb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.6200== Generating with temperature 0.2\n",
      "This movie film is is an a understatement successor its to a roll modern point horror i films still have recommend anyway being the an first independent film horror by movies [UNK] are so filled common that suspend it your makes belief sense in of their proportion darker are aspects gritty of\n",
      "== Generating with temperature 0.5\n",
      "This movie episode did is not fine showcase set idiotic in clichs first that when tiny poor little character guy dies princess in [UNK] successful [UNK] who wish fails is free that of is us also would a roll dangerous of clown sticking with around the pretending face to of never a\n",
      "== Generating with temperature 0.7\n",
      "This movie time starts from off the with third straight of kid the and week sucking line liking laughing getting loudly laughs now immediately this appears movie on may a be dark denied karloff november memory starting this off family just however pops weird up history the way reason back is to\n",
      "== Generating with temperature 1.0\n",
      "This movie movie has is more painful entertaining to and watch sometimes if it you werent want trying to to shut make me the that end they mandy were are elsewhere not for nearly this ten movie minutes just after got about the 30 movie min minimum if so you they cant\n",
      "== Generating with temperature 1.5\n",
      "This movie movie was only one about [UNK] [UNK] men [UNK] and life [UNK] through us very a funny film movie with the accents person unlike and us other concerned american workers actors out in like future popular for culture a and bus when trip people was start too it old can\n",
      "391/391 [==============================] - 84s 212ms/step - loss: 3.6200\n",
      "Epoch 2/250\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.6190== Generating with temperature 0.2\n",
      "This movie though is not worth part watching my it [UNK] is atmosphere a ice biological house turkey is and used the metaphysical story quote other development images [UNK] are [UNK] typical sometimes two you guns will in been the changed next classics touch and the dont [UNK] really  appeals to\n",
      "== Generating with temperature 0.5\n",
      "This movie is is so without bad my that friend so uwe said boll that is hes not making a movies horror ive movies decided had to a die staircase [UNK] we so laughed we that laughed it through was the so [UNK] bad while it the would acting probably was because\n",
      "== Generating with temperature 0.7\n",
      "This movie movie looked is good better and than i real expected love gi theater care its to the movie characters was or not [UNK] that female i in was the the woman only expects action they but cant i soon do find west it american also national [UNK] film agreed right\n",
      "== Generating with temperature 1.0\n",
      "This movie movie is was absolutely a the deliberate story period of the fiction grim traditions story would of still been interest edmund in it the even main has stars nothing [UNK] good going script down for no it reason fell that apart they back dont only even for make a much\n",
      "== Generating with temperature 1.5\n",
      "This movie film [UNK] has a done few way times predictable and atmosphere mediocre those acting included by also veteran the marine [UNK] whose think job batman is returns too to and be the a coolest reasoning got he away hot from property the thanks jungle to tim a [UNK] local in\n",
      "391/391 [==============================] - 84s 214ms/step - loss: 3.6190\n",
      "Epoch 3/250\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.6179== Generating with temperature 0.2\n",
      "This movie movie simply was has bad sexy needless scenes since but it the definitely explicit belongs neither to were me a the big main dance problem attempts was to the be story [UNK] line upon that watching said the i rest jumped of before the killing action the the tire only\n",
      "== Generating with temperature 0.5\n",
      "This movie film was was set shown in in a [UNK] hotel trailer watching i a couldnt blonde tell i the walked exact out opposite of the this way film i failed was in close hill range and the finally bad ended acting was became once so again disappointing this one film\n",
      "== Generating with temperature 0.7\n",
      "This movie [UNK] is stewart far who superior with it precious she little was you an might extraordinary want portrayal to of think [UNK] from culture what we was see to in his india [UNK] in living [UNK] me and and thus placing [UNK] another portion field of all [UNK] white in\n",
      "== Generating with temperature 1.0\n",
      "This movie was only a fifteen western year could old do [UNK] anything [UNK] with in better the picture old if rangers you were will suppose applaud by systems the but way now they actors deserve look to this [UNK] movie 7510 was  to influential abysmal plastic surgery only type of\n",
      "== Generating with temperature 1.5\n",
      "This movie film is represents hilarious one through of the the people directors who have fail done to propaganda realize and that indeed things you are get not a its lot the of topic women during have the been [UNK] different to carried south through america the [UNK] supporting from cast china\n",
      "391/391 [==============================] - 84s 213ms/step - loss: 3.6179\n",
      "Epoch 4/250\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.6174== Generating with temperature 0.2\n",
      "This movie was is 11 partner years ryan old and guy bad speaks 2 russell horrible and cannot ranking believe this the is third only rate one as of one his of best the roles screwball in comedies the like original a gun la but confidential there [UNK] is to to her\n",
      "== Generating with temperature 0.5\n",
      "This movie movie is was a popular feeble gem affair rather to hawn be in warned their or  fake material it doesnt make any sense to any point to movies already begin to do so when paying the bills the car just as good when the met with similar films such\n",
      "== Generating with temperature 0.7\n",
      "This movie stinks was it not should revolutionary have believe been it in change the continuity future of history dinner it and starts you off even with when a you computer think has they a even long older few people skipping have to to think give up this first idea off why\n",
      "== Generating with temperature 1.0\n",
      "This movie is is a very demented reflecting trashy [UNK] often life [UNK] as herself part she of puts leslie this and part [UNK] of arizona social which advances is along not the a value pleasure of a being romantic required myself sarcasm a to time just but get she her so\n",
      "== Generating with temperature 1.5\n",
      "This movie had is something [UNK] in [UNK] that of the wrestling [UNK] heap i [UNK] noticed i that read it who a didnt [UNK] [UNK] [UNK] of lowe then like then he on [UNK] hand joke to [UNK] suspend [UNK] disbelief disbelief to the him ridiculous some take objects from and\n",
      "391/391 [==============================] - 84s 214ms/step - loss: 3.6174\n",
      "Epoch 5/250\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.6165== Generating with temperature 0.2\n",
      "This movie is has simply a one total point of [UNK] view well all thats people all being about separate a it piece has of a pure bright [UNK] color voice this and time young irony men music are and all larger good right than while anyone [UNK] who every wants circle\n",
      "== Generating with temperature 0.5\n",
      "This movie film is was a not lowbudget good jackie enough chan shadow but [UNK] his [UNK] sister still [UNK] has jury very duty enjoyable so little based dark solely disturbing on and some [UNK] serious flight element to is be its really likely obsessed to with spring politics break and and\n",
      "== Generating with temperature 0.7\n",
      "This movie is was gold a when dreadful martin of lawrence the olivier first report version to of the this [UNK] movie the was romance the the 90s script was in very the bad same way old better thin off german often tv wizard movies of hulk oz [UNK] and characters texas\n",
      "== Generating with temperature 1.0\n",
      "This movie movie seems is pretty formidable annoying the to guy behold ritchie on run this no movie martial while arts a tough great gangster actor movie and about the the rest [UNK] of fun the is cast definitely you score all up bunch and of the thin place men to to\n",
      "== Generating with temperature 1.5\n",
      "This movie is is simply not one funny of thing disneys [UNK] about is [UNK] that [UNK] he actually wants [UNK] to any be ethnic accepted fan at and college interesting comedies challenge he from is adding this his film comedy some to of the the scenes [UNK] that clone have the\n",
      "391/391 [==============================] - 84s 214ms/step - loss: 3.6165\n",
      "Epoch 6/250\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.6158== Generating with temperature 0.2\n",
      "This movie movie aired is but still surely touching worth and the the fans special of for this it movie is is not over for the all most time part but was what is america the about creators the and movie the it performances was are quite pretty good good plus there\n",
      "== Generating with temperature 0.5\n",
      "This movie interminable was and i dull found i it gave a it 3 below plus 1 im star not out doing of anything it to shows be im very oh big so why this are movie i [UNK] would between not two enough asian why [UNK] what odd a is pay\n",
      "== Generating with temperature 0.7\n",
      "This movie short is was a probably dud because a the realistic acting portrayal is of undoubtedly james the stewart mother and charles [UNK] evans is did astonishing a performances good by attempt humphrey at [UNK] [UNK] he on doesnt a work [UNK] of which producing flags because when he the begins\n",
      "== Generating with temperature 1.0\n",
      "This movie is is for an people enjoyable who and are a able genius to he be is more not sophisticated just in those this style film of [UNK] action [UNK] music [UNK] is performance usually absolutely as [UNK] usual walking in around space in in castle design and the detail colors\n",
      "== Generating with temperature 1.5\n",
      "This movie is is worthless the every best lacking thing in creativity gags i and love lots at of romantic a comedies dodge the hate senses and yet hate while a theres films some that fx im are not trying rap to charles maintain durning the is envy dark surprise side ending\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 3.6158\n",
      "Epoch 7/250\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.6150== Generating with temperature 0.2\n",
      "This movie flick is tries one hard hard from to bomb understand the the script story will is need weak from acting birth skip to through the the credits toilet go keitel is does humiliating a part shark from attempting the to movie kill crawl me and explains that an no identity\n",
      "== Generating with temperature 0.5\n",
      "This movie sequel might to be that a good good remake one of of the the man first from had the zombies same face zombies as exactly in how one [UNK] bodies suits lisa blow beliefs to 20 use million her in killing a everyone giant outside [UNK] her sound population is\n",
      "== Generating with temperature 0.7\n",
      "This movie movie cracks was me ok i and would loving probably stuart 23 gordon belle had and 15 every pictures others was acted great high diana color is when very she few richard others and was jacqueline very [UNK] lovely it and was this a video favorite this of movie mine\n",
      "== Generating with temperature 1.0\n",
      "This movie film was had horrible great images opening and with keaton pauses as a the shift song to and call not it only a was violent it and a assassination decent three drama words action i its find based most on of the its insane characters to are see there whos\n",
      "== Generating with temperature 1.5\n",
      "This movie was must advertised have as been a a [UNK] [UNK] comedy on about 3 san years francisco after at the school local she one let day alone meet set some in entertaining ireland military who school worked all on in the a [UNK] [UNK] because that everyone an has accident\n",
      "391/391 [==============================] - 85s 217ms/step - loss: 3.6150\n",
      "Epoch 8/250\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.6140== Generating with temperature 0.2\n",
      "This movie after is watching made it me on laugh [UNK] and cinema trying it to is explain clearly why the many psychopath people changed do everything it has is already corny happily and ever unpleasant after to the watch [UNK] but [UNK] i theyre can just probably watch complain it that\n",
      "== Generating with temperature 0.5\n",
      "This movie is was funny billed how but it it shows jumps explains on it the and devil begins images bringing which the could movie well seems have to gone be up a from little a [UNK] compulsive afraid uncertain if but it i doesnt have like enough to time remember is\n",
      "== Generating with temperature 0.7\n",
      "This movie was has incredible great suspenseful [UNK] and with believable the acting twist was at good first too the again story looked was good open and and it executed was real surprisingly  realistic at first you had to think how i could connect it and the concept i must admit\n",
      "== Generating with temperature 1.0\n",
      "This movie latest is the the greatest definitive standup comedy comedy of i any think laugh he my sometimes most consists favorite of of robin most hood lovable plots and are less underrated like repeat to members all and of so course many disney of ideas the were year great their there\n",
      "== Generating with temperature 1.5\n",
      "This movie is made a up show to weve classic all bollywood seen short it films is train apparent wreck that adams there beats is up yet there another with problem the is sure that to this be sort something of prologue a or [UNK] jedi as style viewers of lack what\n",
      "391/391 [==============================] - 84s 215ms/step - loss: 3.6140\n",
      "Epoch 9/250\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.6135== Generating with temperature 0.2\n",
      "This movie is was as pretty stiff [UNK] as was an how especially i cheadle had would elected have to made agree no to such [UNK] 30s description it and may hollywood not could have not magic made found a myself sense tv of reporter the or 1930s the this film film\n",
      "== Generating with temperature 0.5\n",
      "This movie story is was not so bad bright that you it could because be the a big new part thing to was be a [UNK] noname no effort di to director compete john enough tony kung [UNK] fu to flavor 1979 to actor it in in tune approach with to pretty\n",
      "== Generating with temperature 0.7\n",
      "This movie is has a a bad change attempt into to friendship tap only into to a repeat dog of doc other though kinds firsthand of king outlandish plays failed thats science the wreck hysterically of bright high rock school music and california the some script of is the today wedding winter\n",
      "== Generating with temperature 1.0\n",
      "This movie is is being beyond voted me aside least from ten heaven of lost my and temper i when decided a to wonderful go piece [UNK] of to perfection see in it movies is that such this as is nowadays typical this hollywood is blaxploitation pure in 80s the cinema film\n",
      "== Generating with temperature 1.5\n",
      "This movie miniseries has is no one exception soda i in have many seen high this acting monstrosity is and out above of mentioned hope movies if like this rare there because are this no thing people that must may try find less them well the taking story of wide its phenomena\n",
      "391/391 [==============================] - 81s 206ms/step - loss: 3.6135\n",
      "Epoch 10/250\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.6127== Generating with temperature 0.2\n",
      "This movie movie is was a thrown awkward together joins no a good building and a ruins known it parents is dont kinda get like excited this dark movie ironic will when be i very try much to coffee point table dark scene scary i personnel have wanted a the serious [UNK]\n",
      "== Generating with temperature 0.5\n",
      "This movie is was nothing terrible more and than above [UNK] all sex you in got hiphop it and end friday up the taking 13th it with seriously joey called [UNK] duped not into only film being there hbo acting or and the after special all effects it as probably well the\n",
      "== Generating with temperature 0.7\n",
      "This movie move is originally basically really a faithful captain adaptation of of cinderella the robin brooding hood aspect and after although leaving it the for same the circumstances one the of more the sophisticated more the exciting novel one mixes of with comedy the and writing the for film the it\n",
      "== Generating with temperature 1.0\n",
      "This movie is was an well enjoyable cast random and characters we are talked not of to christians laugh everyone out taking loud rebel a against bad the women movie suspected is of this in issue the of first the words story that acting although wasnt is plenty okay of and stunts\n",
      "== Generating with temperature 1.5\n",
      "This movie is was the disappointing star and [UNK] the give plot it that a and [UNK] you but would not enjoy be this disappointed good us movie drama should and be entertain a the total first lack half of of [UNK] the of last the century original watching hair this and\n",
      "391/391 [==============================] - 81s 206ms/step - loss: 3.6127\n",
      "Epoch 11/250\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.6119== Generating with temperature 0.2\n",
      "This movie isnt was [UNK] likely by to arthur behave the anything way [UNK] there his is concern a for few his hours neurotic or through rights allan involvement an or actress crazed well lights what the follows vast is green a the b thick grade of [UNK] all not of much\n",
      "== Generating with temperature 0.5\n",
      "This movie is from a [UNK] metaphor who of is a almost significant deserted one islands that for is one no of one those as 3 those [UNK] three that of explores the the clichs mysterious which princess i does think not one sad of to the [UNK] amount words of cannot\n",
      "== Generating with temperature 0.7\n",
      "This movie is managed the to worst feel as of no what stars was cast with but potential could this not movie inferno it director is gets okay dumber beaten because by the a fact poor is script that jumps plays from questionable chinese needs to a zen similar fantastic to movie\n",
      "== Generating with temperature 1.0\n",
      "This movie is goes one into too the many middle plot [UNK] is of transported a through [UNK] a in genius which and is his different perfect stories children could hours have in turned achieving into it a very magical smart movement dialogs that it forms portrays out the very literary dramatic\n",
      "== Generating with temperature 1.5\n",
      "This movie film is is just way stunned overrated and and utterly misunderstood unbelievably by hilariously todays oscar creativity corporate the office dialogue is is less still cliched in im the amazed [UNK] and fantastic shock performance travolta is who a gets [UNK] anyone playing likes the this lives film can will\n",
      "391/391 [==============================] - 81s 205ms/step - loss: 3.6119\n",
      "Epoch 12/250\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.6110== Generating with temperature 0.2\n",
      "This movie could focused have on been the [UNK] worst and movie i i have have seen ever a the [UNK] director arc thought was this actor could was play absolutely quite disgusting well the just choreography say was why better did thats they because even the the two beautiful performances scenes\n",
      "== Generating with temperature 0.5\n",
      "This movie film is deserved a by lot history people this whove gives documentaries this but movie extremely is different set than in an brazil extended setting restoration up area is that amazing of and course i proving believe that it it was would on never par say with it western didnt\n",
      "== Generating with temperature 0.7\n",
      "This movie was was so not much disappointed movie in is excitement that it is is 1 not boring funny a it mystery would a surprise lot you plots totally have ruin various it scenes is for anything you the havent acting already outstanding already cinematography youve peter guessed [UNK] it are\n",
      "== Generating with temperature 1.0\n",
      "This movie must was have always the looked same great its way too better clean than im most immensely of fond the memories novels now i and love it the the props movie owners illustrates hard the to movie read from make pages that true this it [UNK] does the not same\n",
      "== Generating with temperature 1.5\n",
      "This movie movie deserves is a an very oscar well for done best jamie of kennedy these the is cast all including time the and setup basically that tries senator to becomes big [UNK] in war the or stars to in his fact investigation he when does he away makes happens it\n",
      "391/391 [==============================] - 81s 206ms/step - loss: 3.6110\n",
      "Epoch 13/250\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.6107== Generating with temperature 0.2\n",
      "This movie movie is is one not of so [UNK] many carbon pauly [UNK] shore latest as movies his like younger this da one [UNK] was that a they lazy owes number dirty zealand deeds transforms everybody into is offbeat a romantic very triangle good in middle this class story comedy and\n",
      "== Generating with temperature 0.5\n",
      "This movie is was amazing bad silly heres and why murder the is plot is lines so like embarrassing such jokes as are the [UNK] audience but is try gong to li challenge they [UNK] do him to in try time to and act make like fun quantity into rent par the\n",
      "== Generating with temperature 0.7\n",
      "This movie is was so hailed overblown in and spain at as its a circus standout of baxter admit resume that quality is of both an actors actress at in the her thumb thirties while with her rape presence the over performance about and a shes painter now obsessed taking with the\n",
      "== Generating with temperature 1.0\n",
      "This movie movie is was one unique of hit the made usual for gangster wood movies its although short it title was is key typical to of plot die twists hard etc does just but combine gritty humor gags and mixture stylish decent horror performances and on memorable top situations of several\n",
      "== Generating with temperature 1.5\n",
      "This movie movie is has a a brilliant lot [UNK] of and final many crush sequels the can three not plus weeks it of was bad expected but there nonetheless are it two might of mess effect some but seem even to better swell remember thing being of said both and on\n",
      "391/391 [==============================] - 81s 206ms/step - loss: 3.6107\n",
      "Epoch 14/250\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.6097== Generating with temperature 0.2\n",
      "This movie film is was directed very by well the written director [UNK] of could all make be one familiar besides with sometimes a corny story yet about most the serial clips killer make and it some is of trying the take strangest on away the from camera the to film his\n",
      "== Generating with temperature 0.5\n",
      "This movie story is craft initially demi fantastic that billie in is her beautiful beginning and with when mediocre the songs scene were are sung awesome over and a over lack your of loneliness mom who that is she right acted as so any well other to ones show what of would\n",
      "== Generating with temperature 0.7\n",
      "This movie movie was is like nothing a more pointless movie movie far that off does of have good a movie nice company acting but is it not the scary only parts reason made to this play movie out was 20 because years it later should the have [UNK] no  beyond\n",
      "== Generating with temperature 1.0\n",
      "This movie is seems horrible to it be people just who have have stated no that one it has elevator him readers at have what a he wish may to have be made got because sick it in is all one he example is that in a the flashback first scenes three\n",
      "== Generating with temperature 1.5\n",
      "This movie film riddled was with cheese idiotic and scenes the that [UNK] made i me think feel about like change a for disease us i time think to its really best one ripping or the not cake yet not some even other here ways give would it tolerate a is little\n",
      "391/391 [==============================] - 81s 206ms/step - loss: 3.6097\n",
      "Epoch 15/250\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.6093== Generating with temperature 0.2\n",
      "This movie is as much good a as filmnoir i russian think shallow he gregory cannot peck well is still a helping massive man the who love had chaney problems in with his his voice eyes number he one presents cherry him de more us and more stacey about has in some\n",
      "== Generating with temperature 0.5\n",
      "This movie may was not a [UNK] commercial actually but i others [UNK] have give to 4 each out worst last of the all universal time films viewers were agree to with portray far the superior worst tale attempt of by a american [UNK] director suffice who it as to a say\n",
      "== Generating with temperature 0.7\n",
      "This movie must is have definitely stopped 15 there votes are for a worthy good want script to this fight nice story acting beautiful in believable the direction plot and and acting what but i in think the i world must love assume to the see people within whom a who movie\n",
      "== Generating with temperature 1.0\n",
      "This movie movie was had a never completely mind run the a gamut woman of coming shallow from uninteresting her and character uninteresting became character muddy was tension also nothing [UNK] short character development found were too chilling [UNK] for [UNK] recently is in what the appears movie to tries be to\n",
      "== Generating with temperature 1.5\n",
      "This movie film was has working so as many a of crew us at are the not usa right anymore then with there good are documentaries serious that side stay trip with to them sea where for family the meaning last of weekend their i will can classify give better birth [UNK]\n",
      "391/391 [==============================] - 81s 206ms/step - loss: 3.6093\n",
      "Epoch 16/250\n",
      "391/391 [==============================] - ETA: 0s - loss: 3.6085== Generating with temperature 0.2\n",
      "This movie film is out really there bad isnt movie so i negative feel plays bad at its the best 80s plot roots idea can but you its like not no least the its core so ive bad not football quite but because i its feel so at bad the this level\n",
      "== Generating with temperature 0.5\n",
      "This movie film is concerns what life utterly both lives are with pretty we good sent potential to horribly learn badly to james look dean about cameron his mitchell shared and church [UNK] he apparently was decide looking for for his the new steel team investigator of says [UNK] must is go\n",
      "== Generating with temperature 0.7\n",
      "This movie movie was is terrible the the producers accents came are out almost of never the i same wasted message the of 2 this interesting movie twists was my made all me the realize plot is was that rewritten poorly by written several turns film it filming really some the action\n",
      "== Generating with temperature 1.0\n",
      "This movie is introduces tacky two the of main them [UNK] to a protect national park security mate guard a who [UNK] can with [UNK] his realizes daughter that [UNK] she seeing is others sent party to without [UNK] a which phone he some looks wonders like about a 18 dead year\n",
      "== Generating with temperature 1.5\n",
      "This movie movie started goes out as so a many whole other moments people once need you to to see see [UNK] them  [UNK] and kungfu [UNK] barely known in every other number number 1 way first of all it still boggles my mind what was that his name is horrendous\n",
      "391/391 [==============================] - 81s 205ms/step - loss: 3.6085\n",
      "Epoch 17/250\n",
      " 45/391 [==>...........................] - ETA: 1:06 - loss: 3.6240"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/dense_2/BiasAdd' defined at (most recent call last):\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\kaspe\\AppData\\Local\\Temp\\ipykernel_16448\\758589634.py\", line 1, in <module>\n      model.fit(lm_dataset, epochs=250, callbacks=callbacks)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\layers\\core\\dense.py\", line 252, in call\n      outputs = tf.nn.bias_add(outputs, self.bias)\nNode: 'model/dense_2/BiasAdd'\nOOM when allocating tensor with shape[256,99,15000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/dense_2/BiasAdd}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_15652468]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'model/dense_2/BiasAdd' defined at (most recent call last):\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\kaspe\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\kaspe\\AppData\\Local\\Temp\\ipykernel_16448\\758589634.py\", line 1, in <module>\n      model.fit(lm_dataset, epochs=250, callbacks=callbacks)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\kaspe\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\layers\\core\\dense.py\", line 252, in call\n      outputs = tf.nn.bias_add(outputs, self.bias)\nNode: 'model/dense_2/BiasAdd'\nOOM when allocating tensor with shape[256,99,15000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/dense_2/BiasAdd}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_15652468]"
     ]
    }
   ],
   "source": [
    "model.fit(lm_dataset, epochs=186, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "njj24vtm-q4sv-9ret-pej4-v503ysf3ngc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Käyttöönotto\r\n",
    "Koska malli on tallennettu `text_gen.keras` tiedostoon, sen voi ottaa käyttöön `keras.load_model` funktiolla. Joudut asettamaan `custom_objects` sanakirjan, jonka avulla malli osaa käyttää luomiamme luokkia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44507e36-5c2c-4237-9bde-659c0b46e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"text_gen.keras\", custom_objects={\"PositionalEmbedding\": PositionalEmbedding, \"TransformerDecoder\": TransformerDecoder})"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "rpbzlyxu-4tbf-yvnj-xmh0-ngza6glhn6xh",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tässä on hyvin yksinkertainen tapa luoda lauseita. Asetamme lauseen alun mallille, jonka jälkeen generoimme `1.5` lämpötilalla seuraavat `100` sanaa. Kuten näemme ulos tuleva lause on täysin sekava, mutta kyllä niistä välillä hyvät naurut saa aikaan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "544c8426-acf3-4650-9955-4f9b9e7e0b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This horror movie was fabulous cult is absolutely it trash amazing awful rock just silly fashionable ridiculous dull cunning fun darn morbid its genetic wicked flick psychopathic book broke 100 typing visually heart mistake suspect anyways ladies major that [UNK] chad crappy cairo belongs dorothy appears blaming every cleese no jason viewer is make disgraceful there zoom a modernday all ripley must edge blown once miss of repulsive by what your scare hype does good started a a use youre big killing in brings unacceptable  a out right body to below parts serbian levels but by rural worse any depravity chevy movie he\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This horror movie was\"\n",
    "temperature = 1.5\n",
    "\n",
    "for i in range(100):\n",
    "    tokenized_sentence = text_vectorization([sentence])\n",
    "    predictions = model(tokenized_sentence)\n",
    "    next_token = sample_next(predictions[0, i, :], temperature)\n",
    "    sampled_token = tokens_index[next_token]\n",
    "    sentence += \" \" + sampled_token\n",
    "print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
