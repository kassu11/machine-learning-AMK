{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "86b3cc58-3862-4f58-9dc6-d51c8206990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutustuminen transformer-arkkitehtuuriin\r\n",
    "## Tavoitteet\r\n",
    "Tavoitteena on luoda malli, jolla voimme kääntää tekstiä englannista ranskaksi käyttämällä transformereita.\r\n",
    "## Datan kuvaus\r\n",
    "Datasetti pitää sisällään yli 100 000 elokuva-arvostelua, jotka on jaettu positiivisiksi ja negatiivisiksi arvosteluiksi. Data on tekstimuodossa, ja se täytyy jakaa tekstipareihin tätä tarkoitusta varten."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "xcne6slm-rmrg-jis6-y88t-ay2n1i3qp1mr",
   "metadata": {},
   "outputs": [],
   "source": [
    "Otamme `GPU` koulutuksen käyttöön, jos laite sitä tukee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f8df40-f41d-446e-bbb6-6476b9876e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "0qxx59m5-f8pz-8l0f-fizg-mccm1c3xv6l1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Luemme datasetin läpi rivi kerrallaan. Jokaiselta riviltä otamme englanninkielisen ja ranskankielisen sanaparin ja lisäämme sen `text_pairs`-muuttujaan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "epjequ3l-d4z1-a9tc-jy6i-k98btk9t3msz",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\r\n",
    "from keras.layers import TextVectorization, Input, Embedding, LSTM, Dense\r\n",
    "\r\n",
    "text_file = \"fra-eng/fra.txt\" \r\n",
    "with open(text_file, encoding='utf-8') as f:\r\n",
    "    lines = f.read().split(\"\\n\")[:-1]\r\n",
    "text_pairs = [] \r\n",
    "for line in lines:                              \r\n",
    "    english, french, license = line.split(\"\\t\")\r\n",
    "    french = \"[start] \" + french + \" [end]\"\r\n",
    "    text_pairs.append((english, french))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8rntluam-56lo-6iju-8jtg-0vjk409f8yf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Go.', '[start] Va ! [end]'), ('Go.', '[start] Marche. [end]'), ('Go.', '[start] En route ! [end]'), ('Go.', '[start] Bouge ! [end]'), ('Hi.', '[start] Salut ! [end]')]\n"
     ]
    }
   ],
   "source": [
    "print(text_pairs[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "33pfjjit-4sxn-edmz-l275-u22lvb0cn6q4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Datan Esikäsittely\r\n",
    "Hyvän koulutuksen saavuttamisen kannalta on nyt hyvä sekoittaa datasetti. Asetamme `random.seed(10)`, koska haluamme datan menevän aina samalla tavalla sekaisin, jotta voimme kouluttaa mallia aina samanlaisella datalla.\r\n",
    "Samalla jaamme sekoitetun datan `train`, `test` ja `val` parisetteihin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "yfvlkzzz-fpyh-9f4k-2x3q-katxyfr662s5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(10)\r\n",
    "random.shuffle(text_pairs)\r\n",
    "num_val_samples = int(0.15 * len(text_pairs))\r\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\r\n",
    "train_pairs = text_pairs[:num_train_samples]\r\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\r\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "cbomu2ki-910c-v6q5-v8qj-0j771zpdz33q",
   "metadata": {},
   "outputs": [],
   "source": [
    "Malli käsittelee ja vektoroi englannin- ja ranskankieliset tekstit valmisteluvaiheessa käännös- tai tekstimallin koulutusta varten. Sanat ovet rajoitettu käyttämäkllä vain `15000` sanaa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "qfgca8fk-7g52-n0sg-2eej-r8iq3b11cppl",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \r\n",
    "from tensorflow.keras.layers import TextVectorization\r\n",
    "import string\r\n",
    "import re\r\n",
    "  \r\n",
    "strip_chars = string.punctuation + \"¿\"\r\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\r\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\r\n",
    " \r\n",
    "def custom_standardization(input_string):\r\n",
    "    lowercase = tf.strings.lower(input_string)\r\n",
    "    return tf.strings.regex_replace(\r\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\r\n",
    "vocab_size = 15000\r\n",
    "sequence_length = 20                                    \r\n",
    " \r\n",
    "source_vectorization = TextVectorization(\r\n",
    "    max_tokens=vocab_size,\r\n",
    "    output_mode=\"int\",\r\n",
    "    output_sequence_length=sequence_length,\r\n",
    ")\r\n",
    "target_vectorization = TextVectorization(\r\n",
    "    max_tokens=vocab_size,\r\n",
    "    output_mode=\"int\",\r\n",
    "    output_sequence_length=sequence_length + 1,\r\n",
    "    standardize=custom_standardization,\r\n",
    ")\r\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\r\n",
    "train_french_texts = [pair[1] for pair in train_pairs]\r\n",
    "source_vectorization.adapt(train_english_texts)\r\n",
    "target_vectorization.adapt(train_french_texts) "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "95ev5a3p-n918-7fnu-3mko-s5o9fe6653o1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Seuraavassa koodisolussa määritämme kaksi funktiota, joiden avulla voimme muuttaa olemassaolevat sanaparit sanakirjaksi, jota voimme käyttää mallin koulutukseen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5klcl9rs-sj03-axam-42ek-effouuv6f6m2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 \r\n",
    "  \r\n",
    "def format_dataset(eng, fra):\r\n",
    "    eng = source_vectorization(eng)\r\n",
    "    fra = target_vectorization(fra)\r\n",
    "    return ({\r\n",
    "        \"english\": eng,\r\n",
    "        \"french\": fra[:, :-1],                                \r\n",
    "    }, fra[:, 1:])                                             \r\n",
    " \r\n",
    "def make_dataset(pairs):\r\n",
    "    eng_texts, fra_texts = zip(*pairs)\r\n",
    "    eng_texts = list(eng_texts)\r\n",
    "    fra_texts = list(fra_texts)\r\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, fra_texts))\r\n",
    "    dataset = dataset.batch(batch_size)\r\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\r\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()          \r\n",
    " \r\n",
    "train_ds = make_dataset(train_pairs)\r\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "qe1c4ik8-tdil-fxy8-gtyj-lv7mj7mykm7m",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "inputs['french'].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\r\n",
    "     print(f\"inputs['english'].shape: {inputs['english'].shape}\")\r\n",
    "     print(f\"inputs['french'].shape: {inputs['french'].shape}\")\r\n",
    "     print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "dsu8pvvy-7vji-yh89-kl6v-bl03j1hypa0j",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mallinnus\n",
    "Koodi luo kaksisuuntaisen GRU-pohjaisen koodauskerroksen, joka muuntaa englanninkielisen tekstin numeerisesta sekvenssistä tiivistettyyn piilotilaan, jota voidaan käyttää esimerkiksi käännösmallissa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef5ae8cc-5209-4de4-9b38-c67229cc7060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "embed_dim = 256 \n",
    "latent_dim = 256  \n",
    " \n",
    "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
    "encoded_source = layers.Bidirectional(\n",
    "    layers.GRU(latent_dim), merge_mode=\"sum\")(x)  "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "v1j3asil-6yn9-sabo-qys9-9s7pr99hssdx",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rakennamme seuraavassa koodisolussa lopullisen mallin. Malli koostuu `input`-kerroksesta, joka ottaa sisälleen sanavektorin. `Embedding`-kerroksesta, joka yhdistyy `GRU`-kerrokseen. `decoder_gru`-kerrokseen annetaan sisälle aikaisemmassa koodisolussa määritetty `encoded_source` mallinosa. Mallissa on sitten yksi `Dropout`-kerros, jonka jälkeen viimeisenä kerroksena on `Dense`-kerros softmax-aktivaatiolla.\r\n",
    "Lopulta malli luodaan yhdistämällä `source`- ja `past_target`-kerrokset listaan ja antamalla `target_next_step` mallille y:n arvoksi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12111ea6-f65c-4eb9-b653-716710a21dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_target = Input(shape=(None,), dtype=\"int64\", name=\"french\")\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
    "x = decoder_gru(x, initial_state=encoded_source)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "model = keras.Model([source, past_target], target_next_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "7024tj3t-acoq-9cqe-0klo-a36gjaylils1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Seuraava koodisolu määrittelee mallille callback-funktion, joka tallentaa epochin välein aina parhaan mallin `val_loss`-arvon perusteella.\r\n",
    "Annamme mallille myös optimisoijana rmsprop:in ja käytämme loss-funktiona `sparse_categorical_crossentropy`:a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d11b1672-d7de-435a-b694-a3e1961abef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=\"seq2seqrnn.keras\", save_best_only=True, monitor=\"val_loss\"),\n",
    "]\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "8xvbjcdo-vqcx-zi0z-spov-1lqmf273ppsg",
   "metadata": {},
   "outputs": [],
   "source": [
    "Seuraavassa koodisolussa koulutamme mallin 30:llä epochilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db0e38f7-8444-4412-9137-617fb32845fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2428/2428 [==============================] - 100s 38ms/step - loss: 1.4966 - accuracy: 0.4209 - val_loss: 1.2160 - val_accuracy: 0.5039\n",
      "Epoch 2/30\n",
      "2428/2428 [==============================] - 93s 38ms/step - loss: 1.2276 - accuracy: 0.5170 - val_loss: 1.0709 - val_accuracy: 0.5652\n",
      "Epoch 3/30\n",
      "2428/2428 [==============================] - 87s 36ms/step - loss: 1.1157 - accuracy: 0.5611 - val_loss: 1.0279 - val_accuracy: 0.5981\n",
      "Epoch 4/30\n",
      "2428/2428 [==============================] - 86s 36ms/step - loss: 1.0903 - accuracy: 0.5880 - val_loss: 1.0209 - val_accuracy: 0.6131\n",
      "Epoch 5/30\n",
      "2428/2428 [==============================] - 87s 36ms/step - loss: 1.0762 - accuracy: 0.6048 - val_loss: 1.0150 - val_accuracy: 0.6241\n",
      "Epoch 6/30\n",
      "2428/2428 [==============================] - 87s 36ms/step - loss: 1.0650 - accuracy: 0.6169 - val_loss: 1.0105 - val_accuracy: 0.6304\n",
      "Epoch 7/30\n",
      "2428/2428 [==============================] - 86s 35ms/step - loss: 1.0548 - accuracy: 0.6260 - val_loss: 1.0066 - val_accuracy: 0.6352\n",
      "Epoch 8/30\n",
      "2428/2428 [==============================] - 82s 34ms/step - loss: 1.0448 - accuracy: 0.6335 - val_loss: 1.0040 - val_accuracy: 0.6384\n",
      "Epoch 9/30\n",
      "2428/2428 [==============================] - 88s 36ms/step - loss: 1.0359 - accuracy: 0.6393 - val_loss: 1.0010 - val_accuracy: 0.6406\n",
      "Epoch 10/30\n",
      "2428/2428 [==============================] - 86s 36ms/step - loss: 1.0274 - accuracy: 0.6446 - val_loss: 0.9973 - val_accuracy: 0.6441\n",
      "Epoch 11/30\n",
      "2428/2428 [==============================] - 81s 33ms/step - loss: 1.0198 - accuracy: 0.6491 - val_loss: 0.9960 - val_accuracy: 0.6448\n",
      "Epoch 12/30\n",
      "2428/2428 [==============================] - 87s 36ms/step - loss: 1.0126 - accuracy: 0.6530 - val_loss: 0.9939 - val_accuracy: 0.6458\n",
      "Epoch 13/30\n",
      "2428/2428 [==============================] - 87s 36ms/step - loss: 1.0062 - accuracy: 0.6563 - val_loss: 0.9922 - val_accuracy: 0.6472\n",
      "Epoch 14/30\n",
      "2428/2428 [==============================] - 82s 34ms/step - loss: 1.0003 - accuracy: 0.6596 - val_loss: 0.9907 - val_accuracy: 0.6475\n",
      "Epoch 15/30\n",
      "2428/2428 [==============================] - 81s 34ms/step - loss: 0.9949 - accuracy: 0.6627 - val_loss: 0.9903 - val_accuracy: 0.6480\n",
      "Epoch 16/30\n",
      "2428/2428 [==============================] - 83s 34ms/step - loss: 0.9899 - accuracy: 0.6650 - val_loss: 0.9883 - val_accuracy: 0.6486\n",
      "\n",
      "..."
     ]
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=30, validation_data=val_ds, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2296b457-c127-4dc3-8908-74d34a32e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model(\"seq2seqrnn.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "u4akp0k7-v1eg-l9bi-dklp-t4rnox94o9tn",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nyt voimme tutkia mallin tuottamia käännöksiä."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "449dc055-2653-442c-9f45-7e865abac921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "I know what you could've done.\n",
      "[start] je sais ce que tu pouvais faire [end]\n",
      "-\n",
      "Honesty pays in the long run.\n",
      "[start] lhonnêteté [UNK] à [UNK] [end]\n",
      "-\n",
      "Would you come with us?\n",
      "[start] [UNK] avec nous [end]\n",
      "-\n",
      "Tom winced.\n",
      "[start] tom a fait la [UNK] [end]\n",
      "-\n",
      "How long are you planning on staying?\n",
      "[start] combien de temps vous [UNK] de rester [end]\n",
      "-\n",
      "You're incredibly talented.\n",
      "[start] tu es incroyablement talentueuse [end]\n",
      "-\n",
      "The cutlery belongs in the top drawer.\n",
      "[start] les [UNK] sont dans le tiroir du bureau [end]\n",
      "-\n",
      "I don't want you to be upset.\n",
      "[start] je ne veux pas que vous soyez contrariée [end]\n",
      "-\n",
      "I need a rest.\n",
      "[start] jai besoin de te reposer [end]\n",
      "-\n",
      "This dishtowel is soaking wet.\n",
      "[start] cette [UNK] est [UNK] [end]\n",
      "-\n",
      "It's clear they thought I was somebody else.\n",
      "[start] cest tout ce qui a été dit quil avait rien de faire [end]\n",
      "-\n",
      "If only I could speak French.\n",
      "[start] si je ne savais pas parler français [end]\n",
      "-\n",
      "It's all right.\n",
      "[start] cest tout [end]\n",
      "-\n",
      "I've always distrusted you.\n",
      "[start] je vous [UNK] toujours à une [UNK] [end]\n",
      "-\n",
      "\"When does your sister come back from work?\" \"I don't know, but I think she'll arrive at home a few minutes before me.\"\n",
      "[start] quand estce que je vous ai fait de sa maison si je [UNK] à la maison mais je ne lai\n",
      "-\n",
      "The rainy season begins towards the end of June.\n",
      "[start] la saison des [UNK] est terminée à la fin de [UNK] [end]\n",
      "-\n",
      "It's at the corner.\n",
      "[start] cest au coin [end]\n",
      "-\n",
      "He finally bent to my wishes.\n",
      "[start] il a fait [UNK] mes choses à [UNK] [end]\n",
      "-\n",
      "Do you remember?\n",
      "[start] tu te souviens [end]\n",
      "-\n",
      "It's not just a theory.\n",
      "[start] ce nest pas une théorie [end]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "fra_vocab = target_vectorization.get_vocabulary()                          \n",
    "fra_index_lookup = dict(zip(range(len(fra_vocab)), fra_vocab))             \n",
    "max_decoded_sentence_length = 20\n",
    " \n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"                                           \n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
    "        next_token_predictions = seq2seq_rnn.predict(                      \n",
    "            [tokenized_input_sentence, tokenized_target_sentence], verbose=0)         \n",
    "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])   \n",
    "        sampled_token = fra_index_lookup[sampled_token_index]              \n",
    "        decoded_sentence += \" \" + sampled_token                            \n",
    "        if sampled_token == \"[end]\":                                       \n",
    "            break\n",
    "    return decoded_sentence\n",
    "  \n",
    "test_eng_texts = [pair[0] for pair in test_pairs] \n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "9966b966-3632-4f76-8f2c-ea8814a581d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Arviointi\r\n",
    "Suurin osa mallin tuottamista käännöksistä on tarkkoja ja selkeitä lukuunottamatta puuttuvia sanoja ja pieniä kielioppivirheitä. Lauseet ovat sellaisia, että ranskaa puhuva henkilö saisi niistä kuitenkin helposti selvää. Poikkeuksena on muutama lause, jotka eivät tarkoita yhtään mitään. Mitä pidempiä englanninkielisiä lauseita yritämme kääntää, sitä huonompia käännökset tuntuvat olevan. Rajoitetun sanamäärän `15000` vuoksi usea sana näyttäisi olevan `[UNK]`.\r\n",
    "Käännösten arvioijana toimi ranskaa äidinkielenä puhuva ryhmän jäsen."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "058168da-ebfc-4a34-ac63-d602fced2b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Käyttöönotto\r\n",
    "Mallia voisi tietenkin hyöydyntää omana kääntösovelluksena, mutta mallin tuottamat käännökset voitaisiin implementoida suoraan johonkin tuotteeseen, jota halutaan tarjota useilla kielillä. Esimerkiksi jonkun videonpelin käännöksen voisi hoitaa samanlaisella mallilla, kunhan malli olisi koulutettu sopivalla datalla."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}