{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86b3cc58-3862-4f58-9dc6-d51c8206990a",
   "metadata": {},
   "source": [
    "# Tutustuminen transformer-arkkitehtuuriin\r\n",
    "## Tavoitteet\r\n",
    "Tavoitteena on luoda malli, jolla voimme kääntää tekstiä englannista ranskaksi käyttämällä transformereita.\r\n",
    "## Datan kuvaus\r\n",
    "Datasetti pitää sisällään yli 100 000 elokuva-arvostelua, jotka on jaettu positiivisiksi ja negatiivisiksi arvosteluiksi. Data on tekstimuodossa, ja se täytyy jakaa tekstipareihin tätä tarkoitusta varten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xcne6slm-rmrg-jis6-y88t-ay2n1i3qp1mr",
   "metadata": {},
   "source": [
    "Otamme `GPU` koulutuksen käyttöön, jos laite sitä tukee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f8df40-f41d-446e-bbb6-6476b9876e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0qxx59m5-f8pz-8l0f-fizg-mccm1c3xv6l1",
   "metadata": {},
   "source": [
    "Luemme datasetin läpi rivi kerrallaan. Jokaiselta riviltä otamme englanninkielisen ja ranskankielisen sanaparin ja lisäämme sen `text_pairs`-muuttujaan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "epjequ3l-d4z1-a9tc-jy6i-k98btk9t3msz",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\r\n",
    "from keras.layers import TextVectorization, Input, Embedding, LSTM, Dense\r\n",
    "\r\n",
    "text_file = \"fra-eng/fra.txt\" \r\n",
    "with open(text_file, encoding='utf-8') as f:\r\n",
    "    lines = f.read().split(\"\\n\")[:-1]\r\n",
    "text_pairs = [] \r\n",
    "for line in lines:                              \r\n",
    "    english, french, license = line.split(\"\\t\")\r\n",
    "    french = \"[start] \" + french + \" [end]\"\r\n",
    "    text_pairs.append((english, french))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8rntluam-56lo-6iju-8jtg-0vjk409f8yf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Go.', '[start] Va ! [end]'), ('Go.', '[start] Marche. [end]'), ('Go.', '[start] En route ! [end]'), ('Go.', '[start] Bouge ! [end]'), ('Hi.', '[start] Salut ! [end]')]\n"
     ]
    }
   ],
   "source": [
    "print(text_pairs[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33pfjjit-4sxn-edmz-l275-u22lvb0cn6q4",
   "metadata": {},
   "source": [
    "## Datan Esikäsittely\r\n",
    "Hyvän koulutuksen saavuttamisen kannalta on nyt hyvä sekoittaa datasetti. Asetamme `random.seed(10)`, koska haluamme datan menevän aina samalla tavalla sekaisin, jotta voimme kouluttaa mallia aina samanlaisella datalla.\r\n",
    "Samalla jaamme sekoitetun datan `train`, `test` ja `val` parisetteihin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "yfvlkzzz-fpyh-9f4k-2x3q-katxyfr662s5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(10)\r\n",
    "random.shuffle(text_pairs)\r\n",
    "num_val_samples = int(0.15 * len(text_pairs))\r\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\r\n",
    "train_pairs = text_pairs[:num_train_samples]\r\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\r\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbomu2ki-910c-v6q5-v8qj-0j771zpdz33q",
   "metadata": {},
   "source": [
    "Malli käsittelee ja vektoroi englannin- ja ranskankieliset tekstit valmisteluvaiheessa käännös- tai tekstimallin koulutusta varten. Sanat ovet rajoitettu käyttämäkllä vain `15000` sanaa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "qfgca8fk-7g52-n0sg-2eej-r8iq3b11cppl",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \r\n",
    "from tensorflow.keras.layers import TextVectorization\r\n",
    "import string\r\n",
    "import re\r\n",
    "  \r\n",
    "strip_chars = string.punctuation + \"¿\"\r\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\r\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\r\n",
    " \r\n",
    "def custom_standardization(input_string):\r\n",
    "    lowercase = tf.strings.lower(input_string)\r\n",
    "    return tf.strings.regex_replace(\r\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\r\n",
    "vocab_size = 15000\r\n",
    "sequence_length = 20                                    \r\n",
    " \r\n",
    "source_vectorization = TextVectorization(\r\n",
    "    max_tokens=vocab_size,\r\n",
    "    output_mode=\"int\",\r\n",
    "    output_sequence_length=sequence_length,\r\n",
    ")\r\n",
    "target_vectorization = TextVectorization(\r\n",
    "    max_tokens=vocab_size,\r\n",
    "    output_mode=\"int\",\r\n",
    "    output_sequence_length=sequence_length + 1,\r\n",
    "    standardize=custom_standardization,\r\n",
    ")\r\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\r\n",
    "train_french_texts = [pair[1] for pair in train_pairs]\r\n",
    "source_vectorization.adapt(train_english_texts)\r\n",
    "target_vectorization.adapt(train_french_texts) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ev5a3p-n918-7fnu-3mko-s5o9fe6653o1",
   "metadata": {},
   "source": [
    "Seuraavassa koodisolussa määritämme kaksi funktiota, joiden avulla voimme muuttaa olemassaolevat sanaparit sanakirjaksi, jota voimme käyttää mallin koulutukseen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5klcl9rs-sj03-axam-42ek-effouuv6f6m2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 \r\n",
    "  \r\n",
    "def format_dataset(eng, fra):\r\n",
    "    eng = source_vectorization(eng)\r\n",
    "    fra = target_vectorization(fra)\r\n",
    "    return ({\r\n",
    "        \"english\": eng,\r\n",
    "        \"french\": fra[:, :-1],                                \r\n",
    "    }, fra[:, 1:])                                             \r\n",
    " \r\n",
    "def make_dataset(pairs):\r\n",
    "    eng_texts, fra_texts = zip(*pairs)\r\n",
    "    eng_texts = list(eng_texts)\r\n",
    "    fra_texts = list(fra_texts)\r\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, fra_texts))\r\n",
    "    dataset = dataset.batch(batch_size)\r\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\r\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()          \r\n",
    " \r\n",
    "train_ds = make_dataset(train_pairs)\r\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "qe1c4ik8-tdil-fxy8-gtyj-lv7mj7mykm7m",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "inputs['french'].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\r\n",
    "     print(f\"inputs['english'].shape: {inputs['english'].shape}\")\r\n",
    "     print(f\"inputs['french'].shape: {inputs['french'].shape}\")\r\n",
    "     print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dsu8pvvy-7vji-yh89-kl6v-bl03j1hypa0j",
   "metadata": {},
   "source": [
    "## Mallinnus\n",
    "Koodi luo kaksisuuntaisen GRU-pohjaisen koodauskerroksen, joka muuntaa englanninkielisen tekstin numeerisesta sekvenssistä tiivistettyyn piilotilaan, jota voidaan käyttää esimerkiksi käännösmallissa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef5ae8cc-5209-4de4-9b38-c67229cc7060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "embed_dim = 256 \n",
    "latent_dim = 256  \n",
    " \n",
    "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
    "encoded_source = layers.Bidirectional(\n",
    "    layers.GRU(latent_dim), merge_mode=\"sum\")(x)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v1j3asil-6yn9-sabo-qys9-9s7pr99hssdx",
   "metadata": {},
   "source": [
    "Rakennamme seuraavassa koodisolussa lopullisen mallin. Malli koostuu `input`-kerroksesta, joka ottaa sisälleen sanavektorin. `Embedding`-kerroksesta, joka yhdistyy `GRU`-kerrokseen. `decoder_gru`-kerrokseen annetaan sisälle aikaisemmassa koodisolussa määritetty `encoded_source` mallinosa. Mallissa on sitten yksi `Dropout`-kerros, jonka jälkeen viimeisenä kerroksena on `Dense`-kerros softmax-aktivaatiolla.\r\n",
    "Lopulta malli luodaan yhdistämällä `source`- ja `past_target`-kerrokset listaan ja antamalla `target_next_step` mallille y:n arvoksi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12111ea6-f65c-4eb9-b653-716710a21dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_target = Input(shape=(None,), dtype=\"int64\", name=\"french\")\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
    "x = decoder_gru(x, initial_state=encoded_source)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "model = keras.Model([source, past_target], target_next_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7024tj3t-acoq-9cqe-0klo-a36gjaylils1",
   "metadata": {},
   "source": [
    "Seuraava koodisolu määrittelee mallille callback-funktion, joka tallentaa epochin välein aina parhaan mallin `val_loss`-arvon perusteella.\r\n",
    "Annamme mallille myös optimisoijana rmsprop:in ja käytämme loss-funktiona `sparse_categorical_crossentropy`:a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d11b1672-d7de-435a-b694-a3e1961abef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=\"seq2seqrnn.keras\", save_best_only=True, monitor=\"val_loss\"),\n",
    "]\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8xvbjcdo-vqcx-zi0z-spov-1lqmf273ppsg",
   "metadata": {},
   "source": [
    "Seuraavassa koodisolussa koulutamme mallin 30:llä epochilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db0e38f7-8444-4412-9137-617fb32845fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2428/2428 [==============================] - 100s 38ms/step - loss: 1.4966 - accuracy: 0.4209 - val_loss: 1.2160 - val_accuracy: 0.5039\n",
      "Epoch 2/30\n",
      "2428/2428 [==============================] - 93s 38ms/step - loss: 1.2276 - accuracy: 0.5170 - val_loss: 1.0709 - val_accuracy: 0.5652\n",
      "Epoch 3/30\n",
      "2428/2428 [==============================] - 87s 36ms/step - loss: 1.1157 - accuracy: 0.5611 - val_loss: 1.0279 - val_accuracy: 0.5981\n",
      "Epoch 4/30\n",
      "2428/2428 [==============================] - 86s 36ms/step - loss: 1.0903 - accuracy: 0.5880 - val_loss: 1.0209 - val_accuracy: 0.6131\n",
      "Epoch 5/30\n",
      "2428/2428 [==============================] - 87s 36ms/step - loss: 1.0762 - accuracy: 0.6048 - val_loss: 1.0150 - val_accuracy: 0.6241\n",
      "Epoch 6/30\n",
      "2428/2428 [==============================] - 87s 36ms/step - loss: 1.0650 - accuracy: 0.6169 - val_loss: 1.0105 - val_accuracy: 0.6304\n",
      "Epoch 7/30\n",
      "2428/2428 [==============================] - 86s 35ms/step - loss: 1.0548 - accuracy: 0.6260 - val_loss: 1.0066 - val_accuracy: 0.6352\n",
      "Epoch 8/30\n",
      "2428/2428 [==============================] - 82s 34ms/step - loss: 1.0448 - accuracy: 0.6335 - val_loss: 1.0040 - val_accuracy: 0.6384\n",
      "Epoch 9/30\n",
      "2428/2428 [==============================] - 88s 36ms/step - loss: 1.0359 - accuracy: 0.6393 - val_loss: 1.0010 - val_accuracy: 0.6406\n",
      "Epoch 10/30\n",
      "2428/2428 [==============================] - 86s 36ms/step - loss: 1.0274 - accuracy: 0.6446 - val_loss: 0.9973 - val_accuracy: 0.6441\n",
      "Epoch 11/30\n",
      "2428/2428 [==============================] - 81s 33ms/step - loss: 1.0198 - accuracy: 0.6491 - val_loss: 0.9960 - val_accuracy: 0.6448\n",
      "Epoch 12/30\n",
      "2428/2428 [==============================] - 87s 36ms/step - loss: 1.0126 - accuracy: 0.6530 - val_loss: 0.9939 - val_accuracy: 0.6458\n",
      "Epoch 13/30\n",
      "2428/2428 [==============================] - 87s 36ms/step - loss: 1.0062 - accuracy: 0.6563 - val_loss: 0.9922 - val_accuracy: 0.6472\n",
      "Epoch 14/30\n",
      "2428/2428 [==============================] - 82s 34ms/step - loss: 1.0003 - accuracy: 0.6596 - val_loss: 0.9907 - val_accuracy: 0.6475\n",
      "Epoch 15/30\n",
      "2428/2428 [==============================] - 81s 34ms/step - loss: 0.9949 - accuracy: 0.6627 - val_loss: 0.9903 - val_accuracy: 0.6480\n",
      "Epoch 16/30\n",
      "2428/2428 [==============================] - 83s 34ms/step - loss: 0.9899 - accuracy: 0.6650 - val_loss: 0.9883 - val_accuracy: 0.6486\n",
      "Epoch 17/30\n",
      "  51/2428 [..............................] - ETA: 1:18 - loss: 0.9793 - accuracy: 0.6663"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=30, validation_data=val_ds, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2296b457-c127-4dc3-8908-74d34a32e849",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model(\"seq2seqrnn.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u4akp0k7-v1eg-l9bi-dklp-t4rnox94o9tn",
   "metadata": {},
   "source": [
    "Nyt voimme tutkia mallin tuottamia käännöksiä."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "449dc055-2653-442c-9f45-7e865abac921",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc3 in position 7: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m fra_vocab \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_vectorization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocabulary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m                          \n\u001b[0;32m      3\u001b[0m fra_index_lookup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(fra_vocab)), fra_vocab))             \n\u001b[0;32m      4\u001b[0m max_decoded_sentence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\layers\\preprocessing\\text_vectorization.py:487\u001b[0m, in \u001b[0;36mTextVectorization.get_vocabulary\u001b[1;34m(self, include_special_tokens)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vocabulary\u001b[39m(\u001b[38;5;28mself\u001b[39m, include_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    479\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the current vocabulary of the layer.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;124;03m        returned vocabulary will not include any padding or OOV tokens.\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lookup_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocabulary\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\layers\\preprocessing\\index_lookup.py:385\u001b[0m, in \u001b[0;36mIndexLookup.get_vocabulary\u001b[1;34m(self, include_special_tokens)\u001b[0m\n\u001b[0;32m    382\u001b[0m     keys, values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup_table\u001b[38;5;241m.\u001b[39mexport()\n\u001b[0;32m    383\u001b[0m     vocab, indices \u001b[38;5;241m=\u001b[39m (values, keys) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvert \u001b[38;5;28;01melse\u001b[39;00m (keys, values)\n\u001b[0;32m    384\u001b[0m     vocab, indices \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 385\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_vocab_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    386\u001b[0m         indices\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[0;32m    387\u001b[0m     )\n\u001b[0;32m    388\u001b[0m lookup \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moov_token, \u001b[38;5;28mzip\u001b[39m(indices, vocab)\n\u001b[0;32m    390\u001b[0m )\n\u001b[0;32m    391\u001b[0m vocab \u001b[38;5;241m=\u001b[39m [lookup[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_size())]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\layers\\preprocessing\\string_lookup.py:416\u001b[0m, in \u001b[0;36mStringLookup._tensor_vocab_to_numpy\u001b[1;34m(self, vocabulary)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tensor_vocab_to_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocabulary):\n\u001b[0;32m    414\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m vocabulary\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m--> 416\u001b[0m         [tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mas_text(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m vocabulary]\n\u001b[0;32m    417\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\layers\\preprocessing\\string_lookup.py:416\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tensor_vocab_to_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocabulary):\n\u001b[0;32m    414\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m vocabulary\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m--> 416\u001b[0m         [\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m vocabulary]\n\u001b[0;32m    417\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow\\python\\util\\compat.py:110\u001b[0m, in \u001b[0;36mas_text\u001b[1;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[0;32m    108\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m bytes_or_text\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(bytes_or_text, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m--> 110\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbytes_or_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected binary or unicode string, got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m bytes_or_text)\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc3 in position 7: unexpected end of data"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "fra_vocab = target_vectorization.get_vocabulary()                          \n",
    "fra_index_lookup = dict(zip(range(len(fra_vocab)), fra_vocab))             \n",
    "max_decoded_sentence_length = 20\n",
    " \n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"                                           \n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
    "        next_token_predictions = seq2seq_rnn.predict(                      \n",
    "            [tokenized_input_sentence, tokenized_target_sentence], verbose=0)         \n",
    "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])   \n",
    "        sampled_token = fra_index_lookup[sampled_token_index]              \n",
    "        decoded_sentence += \" \" + sampled_token                            \n",
    "        if sampled_token == \"[end]\":                                       \n",
    "            break\n",
    "    return decoded_sentence\n",
    "  \n",
    "test_eng_texts = [pair[0] for pair in test_pairs] \n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9966b966-3632-4f76-8f2c-ea8814a581d4",
   "metadata": {},
   "source": [
    "## Arviointi\r\n",
    "Suurin osa mallin tuottamista käännöksistä on tarkkoja ja selkeitä lukuunottamatta puuttuvia sanoja ja pieniä kielioppivirheitä. Lauseet ovat sellaisia, että ranskaa puhuva henkilö saisi niistä kuitenkin helposti selvää. Poikkeuksena on muutama lause, jotka eivät tarkoita yhtään mitään. Mitä pidempiä englanninkielisiä lauseita yritämme kääntää, sitä huonompia käännökset tuntuvat olevan. Rajoitetun sanamäärän `15000` vuoksi usea sana näyttäisi olevan `[UNK]`.\r\n",
    "Käännösten arvioijana toimi ranskaa äidinkielenä puhuva ryhmän jäsen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058168da-ebfc-4a34-ac63-d602fced2b6e",
   "metadata": {},
   "source": [
    "## Käyttöönotto\r\n",
    "Mallia voisi tietenkin hyöydyntää omana kääntösovelluksena, mutta mallin tuottamat käännökset voitaisiin implementoida suoraan johonkin tuotteeseen, jota halutaan tarjota useilla kielillä. Esimerkiksi jonkun videonpelin käännöksen voisi hoitaa samanlaisella mallilla, kunhan malli olisi koulutettu sopivalla datalla."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
