{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4vz3fkr4-6y48-ikh3-3e2j-e7lv8tym41mi",
   "metadata": {},
   "source": [
    "## Tekstin Luokittelu\r\n",
    "\r\n",
    "## Datan kuvaus\r\n",
    "Datasetti pitää sisällään yli 100 000 elokuva-arvostelua, jotka on jaettu positiiivisiksi ja negatiivisiksi arvosteluiksi. Data on tekstimuodossa, ja siitä pitää vain karsia muutamia merkkejä."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lv7utnx3-cfvl-ifgh-tz4a-y098q41smf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\r\n",
    "gpus = tf.config.list_physical_devices('GPU')\r\n",
    "if gpus:\r\n",
    "  try:\r\n",
    "    # Currently, memory growth needs to be the same across GPUs\r\n",
    "    for gpu in gpus:\r\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\r\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\r\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n",
    "  except RuntimeError as e:\r\n",
    "    # Memory growth must be set before GPUs have been initialized\r\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cg3dnps-ypv0-7p4q-66sb-5nhq5thc9g17",
   "metadata": {},
   "source": [
    "## Datan esikäsittely\r\n",
    "Ensimmäinen vaihe on hakea datasetti ja jakaa se `train` ja `val` setteihin. Tiedostot myös sekoitetaan käyttämällä tiettyä seediä, joka on tässä tapauksessa `1337`.\r\n",
    "Poistimme myös manuaalisesti `unsup`-nimisen kansion, joka piti sisällään elokuva-arvosteluita, joita ei oltu suoraan luokiteltu positiivisiksi tai negatiivisiksi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f10f0e8-553b-4fab-abfa-40c53740dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\" \n",
    "train_dir = base_dir / \"train\" \n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)              \n",
    "    num_val_samples = int(0.2 * len(files))         \n",
    "    val_files = files[-num_val_samples:]            \n",
    "    for fname in val_files:                         \n",
    "        shutil.move(train_dir / category / fname, val_dir / category / fname)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "er7giz87-g38a-l6ba-ildg-gxobzznt9i94",
   "metadata": {},
   "source": [
    "Seuraavassa koodisolussa haemme jaetut datasetit omiin muuttujiinsa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99fb4cd5-ca12-4e3a-abcb-ead9f364f417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32 \n",
    "  \n",
    "train_ds = keras.utils.text_dataset_from_directory(     \n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63d40d9-19ce-4365-bd68-41b9015649cc",
   "metadata": {},
   "source": [
    "## Mallinnus\n",
    "\n",
    "Seuraavaksi määritellään 'TransformerEncoder' luokka jonka, tarkoituksena on antaa sanoille konteksti riippuvainen merkitys (Sana saa perinteisen embedded vektorin sijaan konteksti riippuvaisen vektorin). Tämä auttaa mallia ymmärtämään sanojen järjestystä sekä kontekstia kun tehdään tekstin luokittelua. Meidän encoder luokka sisältää call() nimisen metodin joka suoriutuu kun luokasta tehdään objekti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "078048ec-d800-4dfd-9e1e-2e34ad081b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "  \n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "  \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qe6b7v4c-qgg7-my6o-xf2e-u5g11rqo3a7c",
   "metadata": {},
   "source": [
    "`PositionalEmbedding` luokan ideana on toimia `Embedding`-kerroksena, joka enkoodaa tokenit ja niiden sijainnit sanassa. Sijanti on tärkeää tietää lauseiden luonnissa, koska sanojen sijainnit lauseessa voi vaikuttaa suuresti sanan tarkoitukseen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17b012a2-723d-4335-b354-52d6608c4696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "  \n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    " \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    " \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iblexu9a-tksj-6g8s-bsko-7ocesvncvyj2",
   "metadata": {},
   "source": [
    "Seuraavan solun tarkoitus on vektorisoida jokainen datasetti numeroiksi, jotta voimme käyttää niitä mallin kouluttamiseen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b58114a0-41bb-4d93-b426-059d681435ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "  \n",
    "max_length = 600 \n",
    "max_tokens = 20000 \n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,     \n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    " \n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vfa1ozau-8amz-8uje-sa6y-1tic5yy3f937",
   "metadata": {},
   "source": [
    "Luomme seuraavaksi mallin, johon asetamme `input`-kerroksen jälkeen aiemmin luomat `PositionalEmbedding` ja `TransformerEncoder` kerrokset. Näitä kerroksia seuraa yksi `GlobalMaxPooling1D`-kerros, joka karsii korkeimman arvon pois ja tekee shapesta yksiulotteisen. Lisäämme yhden `Dropout` kerroksen, välttääkseen ylioppimista, mutta muuten kyseinen malli on hyvin suoraviivainen.\r\n",
    "\r\n",
    "Tallennamme mallin `full_transformer_encoder.keras` tiedostoon, jotta sitä voi käyttää helpommin koulutuksen jälkeen. Koulutamme mallia `20` epochia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80b9bc78-c43c-4db2-b831-65f9e829f06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " positional_embedding_1 (Pos  (None, None, 256)        5273600   \n",
      " itionalEmbedding)                                               \n",
      "                                                                 \n",
      " transformer_encoder_1 (Tran  (None, None, 256)        543776    \n",
      " sformerEncoder)                                                 \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 256)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,817,633\n",
      "Trainable params: 5,817,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "625/625 [==============================] - 31s 49ms/step - loss: 0.4841 - accuracy: 0.7753 - val_loss: 0.2933 - val_accuracy: 0.8832\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 29s 47ms/step - loss: 0.2342 - accuracy: 0.9095 - val_loss: 0.3675 - val_accuracy: 0.8882\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 29s 47ms/step - loss: 0.1765 - accuracy: 0.9323 - val_loss: 0.7097 - val_accuracy: 0.8388\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 29s 46ms/step - loss: 0.1462 - accuracy: 0.9480 - val_loss: 0.5122 - val_accuracy: 0.8706\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - 29s 46ms/step - loss: 0.1248 - accuracy: 0.9544 - val_loss: 0.3963 - val_accuracy: 0.8894\n",
      "Epoch 6/20\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.1109 - accuracy: 0.9607 - val_loss: 0.3629 - val_accuracy: 0.8832\n",
      "Epoch 7/20\n",
      "625/625 [==============================] - 30s 47ms/step - loss: 0.0939 - accuracy: 0.9678 - val_loss: 0.4183 - val_accuracy: 0.8854\n",
      "Epoch 8/20\n",
      "625/625 [==============================] - 31s 50ms/step - loss: 0.0821 - accuracy: 0.9720 - val_loss: 0.5046 - val_accuracy: 0.8790\n",
      "Epoch 9/20\n",
      "625/625 [==============================] - 29s 47ms/step - loss: 0.0714 - accuracy: 0.9764 - val_loss: 0.4991 - val_accuracy: 0.8742\n",
      "Epoch 10/20\n",
      "625/625 [==============================] - 29s 47ms/step - loss: 0.0630 - accuracy: 0.9782 - val_loss: 0.6234 - val_accuracy: 0.8780\n",
      "Epoch 11/20\n",
      "625/625 [==============================] - 29s 47ms/step - loss: 0.0542 - accuracy: 0.9811 - val_loss: 0.6078 - val_accuracy: 0.8716\n",
      "Epoch 12/20\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.0463 - accuracy: 0.9851 - val_loss: 0.7339 - val_accuracy: 0.8744\n",
      "Epoch 13/20\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.0460 - accuracy: 0.9855 - val_loss: 0.6070 - val_accuracy: 0.8754\n",
      "Epoch 14/20\n",
      "625/625 [==============================] - 29s 46ms/step - loss: 0.0428 - accuracy: 0.9866 - val_loss: 0.6387 - val_accuracy: 0.8744\n",
      "Epoch 15/20\n",
      "625/625 [==============================] - 29s 46ms/step - loss: 0.0377 - accuracy: 0.9879 - val_loss: 0.7420 - val_accuracy: 0.8704\n",
      "Epoch 16/20\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.0359 - accuracy: 0.9880 - val_loss: 0.6681 - val_accuracy: 0.8720\n",
      "Epoch 17/20\n",
      "625/625 [==============================] - 31s 50ms/step - loss: 0.0305 - accuracy: 0.9901 - val_loss: 0.9206 - val_accuracy: 0.8502\n",
      "Epoch 18/20\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.0299 - accuracy: 0.9898 - val_loss: 0.9432 - val_accuracy: 0.8714\n",
      "Epoch 19/20\n",
      "625/625 [==============================] - 29s 47ms/step - loss: 0.0263 - accuracy: 0.9907 - val_loss: 0.8133 - val_accuracy: 0.8722\n",
      "Epoch 20/20\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.0254 - accuracy: 0.9926 - val_loss: 0.8314 - val_accuracy: 0.8626\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.3010 - accuracy: 0.8801\n",
      "Test acc: 0.880\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000 \n",
    "sequence_length = 600 \n",
    "embed_dim = 256 \n",
    "num_heads = 2 \n",
    "dense_dim = 32 \n",
    "  \n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "  \n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\", save_best_only=True)\n",
    "] \n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
    "model = keras.models.load_model(\n",
    "    \"full_transformer_encoder.keras\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder, \"PositionalEmbedding\": PositionalEmbedding}) \n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f59e46-9631-49d5-be97-f0d4e38483c3",
   "metadata": {},
   "source": [
    "## Arviointi\r\n",
    "Tuloksista päätellen malli alkaa ylioppimaan jo ensimmäisen epochin jälkeen, ja saamme parhaan tuloksen samaisella epochilla. Tästä huolimatta saavutamme silti korkean tarkkuus arvon `88%`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jjokb07a-ukr6-mcuu-f8zr-tj8uj2r8bfx7",
   "metadata": {},
   "source": [
    "## Käyttöönotto\r\n",
    "Tätä mallia voisi hyödyntää mihin tahansa suodatukseen. Hyviä esimerkkejä olisi sähköpostin suodatus tai jonkun yrityksen käyttäjäpalautteiden suodatus. Näihin tarkoituksiin pitäisi olla tietenkin omanlaiset datasetit, mutta malli voisi olla samanlainen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
