{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86b3cc58-3862-4f58-9dc6-d51c8206990a",
   "metadata": {},
   "source": [
    "# Tutustuminen transformer-arkkitehtuuriin toimii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f8df40-f41d-446e-bbb6-6476b9876e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 2 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "o0q2cb58-qa19-y16s-a2ax-d6z4hhrn1ysq",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\r\n",
    "from keras.layers import TextVectorization, Input, Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "epjequ3l-d4z1-a9tc-jy6i-k98btk9t3msz",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = \"fra-eng\\\\fra.txt\" \r\n",
    "with open(text_file, encoding='utf-8') as f:\r\n",
    "    lines = f.read().split(\"\\n\")[:-1]\r\n",
    "text_pairs = [] \r\n",
    "for line in lines:                              \r\n",
    "    english, french, license = line.split(\"\\t\")\r\n",
    "    french = \"[start] \" + french + \" [end]\"\r\n",
    "    text_pairs.append((english, french))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8rntluam-56lo-6iju-8jtg-0vjk409f8yf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Go.', '[start] Va ! [end]'), ('Go.', '[start] Marche. [end]'), ('Go.', '[start] En route ! [end]'), ('Go.', '[start] Bouge ! [end]'), ('Hi.', '[start] Salut ! [end]')]\n"
     ]
    }
   ],
   "source": [
    "print(text_pairs[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "yfvlkzzz-fpyh-9f4k-2x3q-katxyfr662s5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(10)\r\n",
    "random.shuffle(text_pairs)\r\n",
    "num_val_samples = int(0.15 * len(text_pairs))\r\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\r\n",
    "train_pairs = text_pairs[:num_train_samples]\r\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\r\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qfgca8fk-7g52-n0sg-2eej-r8iq3b11cppl",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \r\n",
    "from tensorflow.keras.layers import TextVectorization\r\n",
    "import string\r\n",
    "import re\r\n",
    "  \r\n",
    "strip_chars = string.punctuation + \"¿\"\r\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\r\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\r\n",
    " \r\n",
    "def custom_standardization(input_string):\r\n",
    "    lowercase = tf.strings.lower(input_string)\r\n",
    "    return tf.strings.regex_replace(\r\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\r\n",
    "vocab_size = 15000\r\n",
    "sequence_length = 20                                    \r\n",
    " \r\n",
    "source_vectorization = TextVectorization(\r\n",
    "    max_tokens=vocab_size,\r\n",
    "    output_mode=\"int\",\r\n",
    "    output_sequence_length=sequence_length,\r\n",
    ")\r\n",
    "target_vectorization = TextVectorization(\r\n",
    "    max_tokens=vocab_size,\r\n",
    "    output_mode=\"int\",\r\n",
    "    output_sequence_length=sequence_length + 1,\r\n",
    "    standardize=custom_standardization,\r\n",
    ")\r\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\r\n",
    "train_french_texts = [pair[1] for pair in train_pairs]\r\n",
    "source_vectorization.adapt(train_english_texts)\r\n",
    "target_vectorization.adapt(train_french_texts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5klcl9rs-sj03-axam-42ek-effouuv6f6m2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 \r\n",
    "  \r\n",
    "def format_dataset(eng, fra):\r\n",
    "    eng = source_vectorization(eng)\r\n",
    "    fra = target_vectorization(fra)\r\n",
    "    return ({\r\n",
    "        \"english\": eng,\r\n",
    "        \"french\": fra[:, :-1],                                \r\n",
    "    }, fra[:, 1:])                                             \r\n",
    " \r\n",
    "def make_dataset(pairs):\r\n",
    "    eng_texts, fra_texts = zip(*pairs)\r\n",
    "    eng_texts = list(eng_texts)\r\n",
    "    fra_texts = list(fra_texts)\r\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, fra_texts))\r\n",
    "    dataset = dataset.batch(batch_size)\r\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\r\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()          \r\n",
    " \r\n",
    "train_ds = make_dataset(train_pairs)\r\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qe1c4ik8-tdil-fxy8-gtyj-lv7mj7mykm7m",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in train_ds.take(1):\r\n",
    "     print(f\"inputs['english'].shape: {inputs['english'].shape}\")\r\n",
    "     print(f\"inputs['french'].shape: {inputs['french'].shape}\")\r\n",
    "     print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z91xusnf-duia-cu1d-vyiy-jy14lbf9zyj6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256 \r\n",
    "latent_dim = 1024\r\n",
    "model = Sequential([\r\n",
    "    Input(shape=(None,), dtype=\"int64\", name=\"english\"),\r\n",
    "    Embedding(vocab_size, embed_dim, mask_zero=True),\r\n",
    "    LSTM(32, return_sequences=True),\r\n",
    "    Dense(vocab_size, activation=\"softmax\"),\r\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5ae8cc-5209-4de4-9b38-c67229cc7060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "embed_dim = 256 \n",
    "latent_dim = 256  \n",
    " \n",
    "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
    "encoded_source = layers.Bidirectional(\n",
    "    tf.compat.v1.keras.layers.GRU(latent_dim), merge_mode=\"sum\")(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12111ea6-f65c-4eb9-b653-716710a21dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_target = Input(shape=(None,), dtype=\"int64\", name=\"french\")\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
    "decoder_gru = tf.compat.v1.keras.layers.GRU(latent_dim, return_sequences=True)\n",
    "x = decoder_gru(x, initial_state=encoded_source)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b1672-d7de-435a-b694-a3e1961abef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=\"seq2seqrnn.keras\", save_best_only=True, monitor=\"val_loss\"),\n",
    "]\n",
    "seq2seq_rnn.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0e38f7-8444-4412-9137-617fb32845fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_rnn.fit(train_ds, epochs=30, validation_data=val_ds, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ap7k25db-n9aq-2q6b-yz12-2xrduyqegh1d",
   "metadata": {},
   "source": [
    "# Old stuff below\r\n",
    "remove later\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-\r\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f9e36-b20b-4af1-9a02-f44fdf33d660",
   "metadata": {},
   "source": [
    "## Tavoitteet\r\n",
    "\r\n",
    "- Tekstin luokittelu Transformer encoderin avulla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d409ed-4569-49d5-b433-dfc68f79c2f4",
   "metadata": {},
   "source": [
    "## Mallinnus\r\n",
    "asdasdasdasd asdasdasdasdasd ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbfb7f0-58a4-48f4-a7c4-6b0dfb3f1446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",                   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77db2670-da99-4c99-9780-4c16029c5709",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dda4f55-1550-46e4-ab71-e6cc94f8c4a8",
   "metadata": {},
   "source": [
    "### Giga "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6361600c-691c-4aba-822b-e758034e0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_sentence = \"I write, rewrite, and still rewrite again\" \n",
    "#encoded_sentence = text_vectorization(test_sentence)\n",
    "#print(encoded_sentence)\n",
    "#inverse_vocab = dict(enumerate(vocabulary))\n",
    "#print(inverse)\n",
    "#decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "#print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb1188b-3d42-4597-92e9-4fc021feb20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c9ba9e-1b27-43ee-9fdf-e23c8ffb7100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "  \n",
    "base_dir = pathlib.Path(\"C:\\\\Users\\\\nasim\\\\Downloads\\\\aclImdb_v1\\\\aclImdb\")\n",
    "val_dir = base_dir / \"val\" \n",
    "train_dir = base_dir / \"train\" \n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)              \n",
    "    num_val_samples = int(0.2 * len(files))         \n",
    "    val_files = files[-num_val_samples:]            \n",
    "    for fname in val_files:                         \n",
    "        shutil.move(train_dir / category / fname,   \n",
    "                    val_dir / category / fname)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9874e0-6adf-4ced-8c70-59b3b7b9e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "batch_size = 32 \n",
    "  \n",
    "train_ds = keras.utils.text_dataset_from_directory(     \n",
    "    \"C:\\\\Users\\\\nasim\\\\Downloads\\\\aclImdb_v1\\\\aclImdb\\\\train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"C:\\\\Users\\\\nasim\\\\Downloads\\\\aclImdb_v1\\\\aclImdb\\\\val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"C:\\\\Users\\\\nasim\\\\Downloads\\\\aclImdb_v1\\\\aclImdb\\\\test\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6e05f0-9678-48e4-a86f-6613806cb205",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,                               \n",
    "    output_mode=\"multi_hot\",                        \n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)   \n",
    "text_vectorization.adapt(text_only_train_ds)        \n",
    " \n",
    "binary_1gram_train_ds = train_ds.map(               \n",
    "    lambda x, y: (text_vectorization(x), y),        \n",
    "    num_parallel_calls=4)                           \n",
    "binary_1gram_val_ds = val_ds.map(                   \n",
    "    lambda x, y: (text_vectorization(x), y),        \n",
    "    num_parallel_calls=4)                           \n",
    "binary_1gram_test_ds = test_ds.map(                 \n",
    "    lambda x, y: (text_vectorization(x), y),        \n",
    "    num_parallel_calls=4)                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be461ece-acd4-4a28-8686-5d84fe85082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "  \n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3c880f-7575-4e5b-8062-6324a3061660",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_1gram_train_ds.cache(),                   \n",
    "          validation_data=binary_1gram_val_ds.cache(),     \n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_1gram.keras\") \n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7886f557-7f61-4689-9308-77a30a99e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f71ff3-aa26-47ab-b22c-ecd803d4fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    " \n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "          validation_data=binary_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a032a4-c1a3-48f1-b05c-98c6f7a7c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"count\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5d15e-9de8-420a-8190-8a3964584398",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"tf_idf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f573586c-f530-477b-a966-90bd2bc7369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)      \n",
    " \n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    " \n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aa2183-aeee-4b19-9382-e42f688a6ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(1,), dtype=\"string\")   \n",
    "processed_inputs = text_vectorization(inputs)      \n",
    "outputs = model(processed_inputs)                  \n",
    "inference_model = keras.Model(inputs, outputs)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c57b273-1d6d-4789-8537-c06d50d79db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_data = tf.convert_to_tensor([\n",
    "    [\"That was an excellent movie, I loved it.\"],\n",
    "])\n",
    "predictions = inference_model(raw_text_data) \n",
    "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8065bce3-18c4-409e-bec9-e9150a2a0e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 600 \n",
    "max_tokens = 20000 \n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,     \n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    " \n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce7b78-0d53-40f2-9e77-216a9c0038a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "  \n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"embeddings_bidir_gru.keras\") \n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9966b966-3632-4f76-8f2c-ea8814a581d4",
   "metadata": {},
   "source": [
    "### Arviointi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058168da-ebfc-4a34-ac63-d602fced2b6e",
   "metadata": {},
   "source": [
    "### Käyttöönotto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375184d3-60b3-41cf-9381-afff098cbf7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f0fc1-3f75-4bf5-a334-4d8fae431ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
